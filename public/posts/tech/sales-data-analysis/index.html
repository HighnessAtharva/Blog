<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Sales Data Analysis with Python | Atharva Shah</title>
<meta name="keywords" content="python, tutorial">
<meta name="description" content="This tutorial explores data preprocessing, exploratory data analysis, feature engineering, and model building for sales data analysis.">
<meta name="author" content="">
<link rel="canonical" href="https://atharvashah.netlify.app/posts/tech/sales-data-analysis/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.654a1c80c8e26dd6db8000f4efdce1f39ad50ad30541a0531274fff961afd7e8.css" integrity="sha256-ZUocgMjibdbbgAD079zh85rVCtMFQaBTEnT/&#43;WGv1&#43;g=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://atharvashah.netlify.app/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://atharvashah.netlify.app/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://atharvashah.netlify.app/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://atharvashah.netlify.app/apple-touch-icon.png">
<link rel="mask-icon" href="https://atharvashah.netlify.app/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css">
<script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.min.js"></script>



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = ""; 
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9913536001930134" crossorigin="anonymous"></script>
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-216150796-2', 'auto');
	
	ga('send', 'pageview');
}
</script><meta property="og:title" content="Sales Data Analysis with Python" />
<meta property="og:description" content="This tutorial explores data preprocessing, exploratory data analysis, feature engineering, and model building for sales data analysis." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://atharvashah.netlify.app/posts/tech/sales-data-analysis/" />
<meta property="og:image" content="https://atharvashah.netlify.app/blog/sales-data-analysis/cover.webp" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-06-14T01:18:34+05:30" />
<meta property="article:modified_time" content="2023-06-14T01:18:34+05:30" /><meta property="og:site_name" content="Atharva Shah" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://atharvashah.netlify.app/blog/sales-data-analysis/cover.webp" />
<meta name="twitter:title" content="Sales Data Analysis with Python"/>
<meta name="twitter:description" content="This tutorial explores data preprocessing, exploratory data analysis, feature engineering, and model building for sales data analysis."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "üìö All Posts",
      "item": "https://atharvashah.netlify.app/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "üíªTech",
      "item": "https://atharvashah.netlify.app/posts/tech/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Sales Data Analysis with Python",
      "item": "https://atharvashah.netlify.app/posts/tech/sales-data-analysis/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Sales Data Analysis with Python",
  "name": "Sales Data Analysis with Python",
  "description": "This tutorial explores data preprocessing, exploratory data analysis, feature engineering, and model building for sales data analysis.",
  "keywords": [
    "python", "tutorial"
  ],
  "articleBody": "Introduction ABC Private Limited, a retail company, aims to gain valuable insights into their customers‚Äô purchasing habits. By analyzing the provided summary of high-volume product purchase history, which includes customer demographics and product details, ABC can uncover patterns and trends in customer spending across different product categories.\nWe will explore the analysis step-by-step using the test.csv and train.csv files. Through a combination of theoretical explanations and practical demonstrations, we will delve into data preprocessing, exploratory data analysis, feature engineering, and model building. We will be utilizing the power of Python for conducting Exploratory Data Analysis, performing data visualization, and training machine learning models. It is recommended to follow along using a Jupyter Notebook, as it provides an interactive and user-friendly environment for code execution and documentation.\nTo proceed with the analysis, we will make use of various Python libraries, including:\npandas: for data manipulation and analysis matplotlib: data visualization scikit-learn: evaluating and training machine learning models numpy: numerical computations seaborn: enhanced data visualization xgboost: implementing gradient boosting algorithms sklearn: additional machine learning tools and utilities üí° Make sure to have these packages installed in your Python environment. You can install them using pip or conda by running the following commands:\npip install pandas matplotlib scikit-learn numpy seaborn xgboost sklearn Data Preprocessing Let‚Äôs begin with the exercise by importing the necessary modules. You have already installed these in the previous section.\nimport pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import os import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn import metrics from sklearn.preprocessing import LabelEncoder,minmax_scale,scale from sklearn.linear_model import LinearRegression from sklearn.preprocessing import StandardScaler from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor from sklearn.metrics import mean_absolute_error,mean_squared_error from xgboost.sklearn import XGBRegressor Descriptive Statistics CODE\n# STEP 1: LOAD THE DATA sales_df = pd.read_csv('train.csv', encoding='utf-8', sep=',', header=0) # print the shape of the dataset [rows, columns] rows, columns = sales_df.shape print(f'Shape = {rows} rows x{columns} columns') # print the datatype of each column sales_df.dtypes # print the datatype of each column sales_df.head() OUTPUT\nShape = 550068 rows x 12 columns User_ID int64 Product_ID object Gender object Age object Occupation int64 City_Category object Stay_In_Current_City_Years object Marital_Status int64 Product_Category_1 int64 Product_Category_2 float64 Product_Category_3 float64 Purchase int64 dtype: object To start off we did the following:-\nLoaded the data from the ‚Äútrain.csv‚Äù file into the pandas DataFrame called sales_df and used pd.read_csv() function with ‚Äòutf-8‚Äô encoding and ‚Äò,‚Äô separator to read the CSV file. Set header=0 to indicate that the first row contains column names. Printed the shape of the DataFrame to show the number of rows and columns. Retrieved the number of rows and columns using sales_df.shape and stored them in rows and columns variables. Printed the information about the dataset in a formatted string. Checked the data type of each column using sales_df.dtypes to understand the types of features in the dataset. Identified data types such as int64 for integers and object for strings or categorical variables. Printed the first five rows to get an overview of the nature of columns and the sales data. CODE\n# Check descriptive statistics of the dataset sales_df.describe() OUTPUT\nUser_ID Occupation Marital_Status Product_Category_1 count 5.500680e+05 550068.000000 550068.000000 550068.000000\rmean 1.003029e+06 8.076707 0.409653 5.404270\rstd 1.727592e+03 6.522660 0.491770 3.936211\rmin 1.000001e+06 0.000000 0.000000 1.000000\r25% 1.001516e+06 2.000000 0.000000 1.000000\r50% 1.003077e+06 7.000000 0.000000 5.000000\r75% 1.004478e+06 14.000000 1.000000 8.000000\rmax 1.006040e+06 20.000000 1.000000 20.000000\rProduct_Category_2 Product_Category_3 Purchase count 376430.000000 166821.000000 550068.000000 mean 9.842329 12.668243 9263.968713 std 5.086590 4.125338 5023.065394 min 2.000000 3.000000 12.000000 25% 5.000000 9.000000 5823.000000 50% 9.000000 14.000000 8047.000000 75% 15.000000 16.000000 12054.000000 max 18.000000 18.000000 23961.000000 CODE\ncategorical=sales_df.select_dtypes(include=[object]) print(\"Categorical columns:\",categorical.shape[1]) numerical=sales_df.select_dtypes(include=[np.float64,np.int64]) print(\"Numerical columns:\",numerical.shape[1]) # check for missing values in the data sales_df.isnull().sum() OUTPUT\nCategorical columns: 5\rNumerical columns: 7\rUser_ID 0\rProduct_ID 0\rGender 0\rAge 0\rOccupation 0\rCity_Category 0\rStay_In_Current_City_Years 0\rMarital_Status 0\rProduct_Category_1 0\rProduct_Category_2 173638\rProduct_Category_3 383247\rPurchase 0\rdtype: int64 The takeaway from this so far is:-\nIdentified 5 categorical columns and 7 numerical columns in the dataset. Checked for missing values using sales_df.isnull().sum(). Found that Product_Category_2 has 173,638 missing values and Product_Category_3 has 383,247 missing values. No missing values were found in other columns. üí° Missing values can alter the process because they introduce uncertainty and can affect the statistical properties of the data. Missing data can lead to biased analysis, inaccurate results, and reduced performance of machine learning models. Therefore, it is crucial to handle missing values appropriately to ensure the integrity and reliability of the analysis.\nWays to handle missing values:\nFilling them with a suitable value, such as the mean, median, or mode of the column. This approach is used here, where the missing values in ‚ÄòProduct_Category_2‚Äô and ‚ÄòProduct_Category_3‚Äô are filled with their respective means. Remove the rows or columns with missing values, but this can lead to data loss and should be done cautiously. Depending on the context and the significance of the missing values, other advanced techniques like imputation methods (e.g., regression imputation, KNN imputation) or using machine learning models to predict missing values can be employed. As seen above, Product_Category_2 and Product_Category_3 have several values missing, so let‚Äôs deal with it below.\nCODE\n# Fill missing values with mean sales_df['Product_Category_2']=sales_df['Product_Category_2'].fillna(sales_df['Product_Category_2'].mean()) sales_df['Product_Category_3']=sales_df['Product_Category_3'].fillna(sales_df['Product_Category_3'].mean()) # Check for missing values in the data sales_df.isnull().sum() # see which columns have non-repeating vals sales_df.nunique() OUTPUT\nUser_ID 0\rProduct_ID 0\rGender 0\rAge 0\rOccupation 0\rCity_Category 0\rStay_In_Current_City_Years 0\rMarital_Status 0\rProduct_Category_1 0\rProduct_Category_2 0\rProduct_Category_3 0\rPurchase 0\rdtype: int64\rUser_ID 5891\rProduct_ID 3631\rGender 2\rAge 7\rOccupation 21\rCity_Category 3\rStay_In_Current_City_Years 5\rMarital_Status 2\rProduct_Category_1 20\rProduct_Category_2 18\rProduct_Category_3 16\rPurchase 18105\rdtype: int64 Here, to deal with unstructured data and missing values we did the following:-\nMissing values in the columns ‚ÄòProduct_Category_2‚Äô and ‚ÄòProduct_Category_3‚Äô are filled with the mean value of each respective column using the fillna() function. Checked for missing values again using sales_df.isnull().sum() and confirmed that there are no missing values in any column. Utilized sales df.nunique() method to determine the number of distinct values in each column. Understanding the variety and cardinality of each characteristic is aided by this. Outlier Analysis Outliers are data points that deviate significantly from most of the data in a dataset. They can be unusually high or low values that are distant from the central tendency of the distribution. Outliers can occur due to various reasons, such as measurement errors, data entry mistakes, or genuine extreme observations.\nOutliers are harmful because they can distort the statistical analysis and modeling process. They can have a disproportionate impact on statistical measures such as mean and standard deviation, leading to biased estimates. Outliers can also affect the performance of machine learning models by introducing noise and influencing the model‚Äôs decision boundaries.\nBoxplots are a graphical representation that displays the distribution of a dataset, including information about outliers. In a boxplot, the box represents the interquartile range (IQR), the line inside the box represents the median, and the whiskers extend to the minimum and maximum values within a certain range.\nCODE\n# make a boxplot to show outliers, only for numerical columns columns = ['Purchase', 'Product_Category_1', 'Product_Category_2', 'Occupation'] for column in columns: plt.figure() sns.boxplot(data=sales_df[[column]]) OUTPUT\nTo deal with outliers using boxplots, one approach is to identify and remove the outliers based on predetermined thresholds. This can be done by considering values that fall below the lower whisker or above the upper whisker as outliers. Alternatively, instead of removing outliers, they can be treated or transformed using techniques such as winsorization (replacing extreme values with a specified percentile value) or logarithmic transformations.\nIn the provided code, a boxplot is created for each numerical column (‚ÄòPurchase‚Äô, ‚ÄòProduct_Category_1‚Äô, ‚ÄòProduct_Category_2‚Äô, ‚ÄòOccupation‚Äô) using seaborn‚Äôs boxplot function. This allows visual identification of any outliers present in the data for these specific columns.\nConvert Categorical and Range Variables to Integers Categorical variables are variables that represent qualitative attributes or characteristics. They take on a limited number of distinct categories or levels. Examples of categorical variables include gender, occupation, and city category.\nRange variables, also known as continuous variables, represent quantitative attributes that can take on any value within a certain range. Examples of range variables include age, income, and purchase amount.\nConverting categorical and range variables to integers is important for several reasons:\nNumerical representation: Many machine learning algorithms and statistical models require numerical input. By converting categorical and range variables to integers, we can ensure that the data can be processed and analyzed correctly.\nSimplification of calculations: Numeric representation simplifies calculations and computations. It allows for arithmetic operations and comparisons, which are essential in various statistical analyses and modeling techniques.\nStandardization and scaling: Converting variables to integers can help standardize and scale the data. This is particularly important when using certain algorithms that are sensitive to the scale of the input features. Standardization can improve the performance and stability of these algorithms.\nEncoding categorical variables: Converting categorical variables to integers allows us to apply encoding techniques such as one-hot encoding or label encoding.\nCODE\nprint(sales_df['Stay_In_Current_City_Years'].value_counts()) print(sales_df['City_Category'].value_counts()) print(sales_df['Age'].value_counts()) print(sales_df['Gender']) OUTPUT\n1 193821\r2 101838\r3 95285\r4+ 84726\r0 74398\rName: Stay_In_Current_City_Years, dtype: int64\rB 231173\rC 171175\rA 147720\rName: City_Category, dtype: int64\r26-35 219587\r36-45 110013\r18-25 99660\r46-50 45701\r51-55 38501\r55+ 21504\r0-17 15102\rName: Age, dtype: int64\r0 F\r1 F\r2 F\r3 F\r4 M\r..\r550063 M\r550064 F\r550065 F\r550066 F\r550067 F\rName: Gender, Length: 550068, dtype: object CODE\n# Group by Product_Category_1 and calculate the mean purchase price product_category_1_mean = sales_df[['Product_Category_1', 'Purchase']].groupby(['Product_Category_1'], as_index=False).mean().sort_values(by='Purchase', ascending=False) product_category_1_mean.columns = ['Product_Category_1', 'Mean_Purchase_Price'] product_category_1_mean['Mean_Purchase_Price'] = product_category_1_mean['Mean_Purchase_Price'].map('{:.2f}'.format) # Group by Product_Category_2 and calculate the mean purchase price product_category_2_mean = sales_df[['Product_Category_2', 'Purchase']].groupby(['Product_Category_2'], as_index=False).mean().sort_values(by='Purchase', ascending=False) product_category_2_mean.columns = ['Product_Category_2', 'Mean_Purchase_Price'] product_category_2_mean['Mean_Purchase_Price'] = product_category_2_mean['Mean_Purchase_Price'].map('{:.2f}'.format) # Display the results without indexes with pd.option_context('display.float_format', '{:.2f}'.format): print(product_category_1_mean.to_string(index=False)) print(product_category_2_mean.to_string(index=False)) OUTPUT\nProduct_Category_1 Mean_Purchase_Price\r10 19675.57\r7 16365.69\r6 15838.48\r9 15537.38\r15 14780.45\r16 14766.04\r1 13606.22\r14 13141.63\r2 11251.94\r17 10170.76\r3 10096.71\r8 7498.96\r5 6240.09\r11 4685.27\r18 2972.86\r4 2329.66\r12 1350.86\r13 722.40\r20 370.48\r19 37.04\rProduct_Category_2 Mean_Purchase_Price\r10.00 15648.73\r2.00 13619.36\r6.00 11503.55\r3.00 11235.36\r15.00 10357.08\r16.00 10295.68\r8.00 10273.26\r4.00 10215.19\r13.00 9683.35\r17.00 9421.58\r18.00 9352.44\r5.00 9027.82\r11.00 8940.58\r9.84 7518.70\r9.00 7277.01\r14.00 7105.26\r12.00 6975.47\r7.00 6884.68 As demonstrated, we are performing grouping and aggregation operation on the sales data.\nFor Product_Category_1, we group the data by this category and calculate the mean purchase price for each category. The results are sorted in descending order of purchase price. Similarly, for Product_Category_2, we group the data by this category and calculate the mean purchase price for each category. The results are also sorted in descending order of purchase price. The results are displayed without indexes, and the mean purchase prices are formatted to two decimal places for better readability. Purchase Distribution and Correlation Matrix The correlation coefficients between several variables are displayed in a table called a correlation matrix. It serves as a gauge for the strength and direction of a linear relationship between two variables.Each cell in the matrix represents the correlation coefficient between two variables, ranging from -1 to 1. A positive value indicates a positive correlation, meaning that as one variable increases, the other variable tends to increase as well. A negative value indicates a negative correlation, meaning that as one variable increases, the other variable tends to decrease. A value close to 0 indicates a weak or no correlation between the variables.\nAnalyzing the correlation matrix is important because it provides insights into the relationships between variables. It helps in understanding how different features or variables are related to each other and to the target variable. By identifying strong correlations, we can determine which variables have a significant impact on the target variable and may be important for prediction or modeling purposes. Correlation analysis also helps in feature selection, as highly correlated variables may provide redundant information and can be eliminated to simplify the model and improve interpretability.\nIn machine learning models, correlation analysis helps in several ways:\nFeature Selection: Highly correlated features can be removed to avoid multicollinearity, where multiple variables provide similar information, leading to unstable and less interpretable models.\nFeature Engineering: By analyzing correlations, new features can be created by combining or transforming existing features, which can improve the predictive power of the model.\nModel Interpretation: Understanding the correlation between variables helps in interpreting the model‚Äôs coefficients and understanding the direction and strength of the relationships between features and the target variable.\nCODE\n# plot the correlation matrix as a heatmap plt.figure(figsize=(10, 10)) sns.heatmap(matrix, vmax=1, square=True, cmap='BuPu', annot=True) plt.title('Correlation between different features') plt.show() OUTPUT\nBased on the correlation heatmap, we can observe that certain variables have a stronger correlation with the ‚ÄúPurchase‚Äù column. The variables ‚ÄúUser_ID,‚Äù ‚ÄúMarital Status,‚Äù and ‚ÄúProduct_Category_3‚Äù show relatively weak correlation with the purchase power of the customer. Conversely, the variables ‚ÄúOccupation,‚Äù ‚ÄúProduct Category 1,‚Äù and ‚ÄúProduct Category 2‚Äù exhibit a significantly higher impact on the purchase behavior. It‚Äôs important to note that this correlation analysis is based on numerical fields only, and we will be generating an even more insightful heatmap right before preparing the ML model.\nData Visualization The graphical depiction of data using graphs, charts, and other visual components is known as data visualisation and it is a way to visually explore and communicate patterns, relationships, and trends within the data.\nBy plotting different variables against the ‚ÄúPurchase‚Äù variable, trends can be identified through visual analysis.\nCODE\n# Check the purchase distribution with respect to Gender purchase_gender = sales_df.groupby['Gender']('Purchase').sum() / 1000000 # Rename 0 and 1 to Female and Male purchase_gender.index = ['Female', 'Male'] # Plot the purchase distribution by Gender plt.bar(purchase_gender.index, purchase_gender, color=['#F8766D', '#00BFC4'] ) # Add a title plt.title('Purchase Distribution by Gender') # Customize the x-axis and y-axis labels plt.xlabel('Gender') plt.ylabel('Purchase (in millions)') # Display the plot plt.show() OUTPUT\nBased on the data provided in train.csv, males have purchased significantly more than females, almost 3x more. This would lead us to the conclusion that either the data set is skewed, or men are willing to spend much more on products by company ABC. This also speaks about purchase power parity.\nCODE\n# Check the purchase distribution with respect to Age purchase_age = sales_df.groupby['Age']('Purchase').sum() / 1000000 # Assign the modified age labels as the index purchase_age.index = ['0-17', '18-25', '26-35', '36-45', '46-50', '51-55', '55+'] # Plot the purchase distribution by Age plt.bar(purchase_age.index, purchase_age, color=['#4C72B0', '#55A868', '#C44E52', '#8172B2', '#CCB974', '#64B5CD', '#4C4C4C']) # Add a title plt.title('Purchase Distribution by Age') # Customize the x-axis and y-axis labels plt.xlabel('Age') plt.ylabel('Purchase (in millions)') # Display the plot plt.show() OUTPUT\nAgain, this graph makes sense since most of the audience is represented by the centre curve, which gives the appearance of a normal curve. Most of the audience is between the ages of 26 and 35, while young people and the elderly have the lowest levels of purchasing power.\nCODE\n# Check the purchase distribution with respect to City_Category purchase_city = sales_df.groupby['City_Category']('Purchase').sum() / 1000000 purchase_city.index = ['A', 'B', 'C'] # Plot the purchase distribution by City Category as a pie chart plt.pie(purchase_city, labels=purchase_city.index, autopct='%1.1f%%', startangle=90, colors= ['#4C72B0', '#55A868', '#C44E52']) # Add a title plt.title('Purchase Distribution by City Category') # Add a circle at the center of the pie to make it donut-like circle = plt.Circle((0, 0), 0.10, fc='white') plt.gca().add_artist(circle) plt.axis('equal') # Display the plot plt.show() OUTPUT\nWe can determine the percentage distribution of the city category by plotting a pie chart, and this helps us conclude that people from category B cities have the greatest purchasing power and willingness to spend, while category A cities have the fewest purchases made‚Äînearly 1.8 times fewer than city type B.\nCODE\n# Check the purchase distribution with respect to Occupation purchase_occupation = sales_df.groupby['Occupation']('Purchase').sum() / 1000000 # Bar plot plt.bar(purchase_occupation.index, purchase_occupation.values, color='#4C72B0', edgecolor='black') # Set labels and title plt.title('Purchase Distribution by Occupation') plt.xlabel('Occupation Category') plt.ylabel('Purchase (in millions)') # Add data labels on top of the bars for i, value **in** enumerate(purchase_occupation.values): plt.text(i, value + 0.2, f'{value:.2f}', ha='center') # Add a background color to the plot plt.gca().set_facecolor('#F5F5F5') # Adjust spacing plt.tight_layout() # Display the plot plt.show() OUTPUT\nAs various employment categories pay varying incomes, it stands to reason that category 0, 4, and 7 occupations may potentially have the greatest earnings, which would therefore likely result in more expenditure on goods and services.\nBut the lowest purchase amounts are made by jobs in categories 8, 9, and 18.\nThis may be because individuals in these jobs don‚Äôt require the items made by business ABC, or it may be because of the extremely low earnings in the industrial sector.\nCODE\n# Check the purchase distribution with respect to Stay_In_Current_City_Years purchase_stay = sales_df.groupby['Stay_In_Current_City_Years']('Purchase').sum() / 1000000 # Bar plot plt.bar(purchase_stay.index, purchase_stay.values, color='#8B008B', edgecolor='black') # Set labels and title plt.title('Purchase Distribution by Stay in Current City Years') plt.xlabel('Years stayed in Current City') plt.ylabel('Purchase (in millions)') # Add data labels on top of the bars **for** i, value **in** enumerate(purchase_stay.values): ``plt.text(i, value + 0.2, f'{value:.2f}', ha='center') plt.gca().set_facecolor('#F5F5F5') plt.tight_layout() # Display the plot plt.show() OUTPUT\nThis bar graph reflects the needs of newcomers to the city who require products and services to establish themselves. As a result, those who have been in the city for a year have made the most purchases, which quickly decline as they continue to live there.\nCODE\n# check the purchase distribution with respect to Marital_Status purchase_marital = sales_df.groupby['Marital_Status']('Purchase').sum() / 1000000 # Map values of Marital_Status to corresponding labels purchase_marital.index = ['Unmarried', 'Married'] # Bar plot plt.bar(purchase_marital.index, purchase_marital.values, color=['#6495ED', '#FFA07A'] , edgecolor='black') # Set labels and title plt.title('Purchase Distribution by Marital Status') plt.xlabel('Marital Status') plt.ylabel('Purchase (in millions)') # Add data labels on top of the bars for i, value **in** enumerate(purchase_marital.values): ``plt.text(i, value + 0.2, f'{value:.2f}', ha='center') # Add a background color to the plot plt.gca().set_facecolor('#F5F5F5') # Adjust spacing plt.tight_layout() # Display the plot plt.show() OUTPUT\nWhen contrasted to married people, the bias obviously favours single people, probably because of the costs associated with education, travel, and other products and services. Nonetheless, because of financial considerations like family planning and long-term savings, married people may tend to spend less.\nA scatter plot is a type of data visualization that represents the relationship between two variables. It displays individual data points as dots on a two-dimensional graph, with one variable plotted along the x-axis and the other variable plotted along the y-axis. Scatter plots are useful for visualizing the correlation or relationship between two continuous variables.\nTo see the relationship between product category 1 (categorical) and purchase using a scatter plot, we need to convert the categorical variable into a numerical representation. One way to achieve this is by assigning a numeric code to each category. For example, we can map ‚ÄòProduct_Category_1‚Äô values like ‚ÄòCategory A‚Äô to 1, ‚ÄòCategory B‚Äô to 2, and so on.\nCODE\n# Scatter plot plt.scatter(sales_df['Product_Category_1'], sales_df['Purchase'], color='#8A2BE2', edgecolors='black') # Set labels and title plt.xlabel('Product Category 1') plt.ylabel('Purchase') plt.title('Relationship between Purchase and Product Category 1') # Add gridlines plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7) # Add transparency to the scatter points plt.scatter(sales_df['Product_Category_1'], sales_df['Purchase'], color='#8A2BE2',** edgecolors='black', alpha=0.5) plt.legend(['Data Points']) plt.gca().set_facecolor('#F5F5F5') plt.tight_layout() # Display the plot plt.show() OUTPUT\nAs seen above, product categories 4, 13, 19 and 20 have products which are not priced more than 5000 hence these product categories, although numerous will generate the least revenue. On the other hand, product categories 9, 10, 6, and 7 have items with prices that typically start at or exceed 5,000 and can even reach 20,000 or 25,000, generating more revenue for the business. You can also probabilisticlaly determine the name of these categories from the price ranges. Most other categories are somewhere in between.\nCODE\n# Scatter plot plt.scatter(sales_df['Product_Category_2'], sales_df['Purchase'], color='#32CD32' , edgecolors='black') # Set labels and title plt.xlabel('Product Category 2') plt.ylabel('Purchase') plt.title('Relationship between Purchase and Product Category 2') # Add gridlines plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7) # Add transparency to the scatter points plt.scatter(sales_df['Product_Category_2'], sales_df['Purchase'], color='#32CD32' , edgecolors='black', alpha=0.5) # Add a legend plt.legend(['Data Points']) plt.gca().set_facecolor('#F5F5F5') plt.tight_layout() # Display the plot plt.show() OUTPUT\nCODE\n# Plot the most frequently bought products plt.figure(figsize=(10, 5)) plt.bar(top_frequently_bought.index, top_frequently_bought.values, color='#4287f5', edgecolor='black') plt.title('Most Frequently Bought Products') plt.xlabel('Product ID') plt.ylabel('Purchase Count') plt.xticks(rotation=90) plt.tight_layout() # Adjust spacing plt.show() # Plot the products that generated the most revenue plt.figure(figsize=(10, 5)) plt.bar(top_revenue_products.index, top_revenue_products.values/1000000, color='#4287f5', edgecolor='black') plt.title('Products with the Most Revenue') plt.xlabel('Product ID') plt.ylabel('Revenue (in millions)') plt.xticks(rotation=90) plt.tight_layout() # Adjust spacing plt.show() OUTPUT\nThis code is plotting two bar charts. The first chart shows the most frequently bought products, displaying the purchase count for each product. The second chart displays the products that generated the most revenue, showing the revenue (in millions) for each product.\nThese visualizations provide valuable insights into the popular products and their revenue contribution, which can help identify best-selling items and inform business decisions related to inventory management, marketing strategies, and product promotions. This will prove beneficial for the company to analyze their sales data and identify which products are most popular among their customers.\nLabel Encoding and Feature Selection From this section we will be cleaning up the dataset and picking the features and the target for our Machine Learning model. But before moving on, let us describe a few terms.\nLabel encoding is a process of converting categorical variables into numerical format. It assigns a unique numeric label to each category within a variable. This encoding is useful when working with machine learning algorithms that require numerical inputs.\nFeature selection is the process of selecting a subset of relevant features (variables) from the available dataset that are most predictive or informative for the target variable. It helps to improve model performance, reduce overfitting, and enhance interpretability.\nIn machine learning models, the ‚Äútarget‚Äù refers to the variable that the model aims to predict or estimate. It is also known as the dependent variable or the output variable. The target variable represents the outcome or the value we want to predict based on the input features (independent variables) in the model. The model learns patterns and relationships in the input features to make predictions or classifications for the target variable.\nThe steps involved in making a robust ML model are:\nTo set up your machine learning algorithm for predicting the values of the ‚Äúpurchase‚Äù column based on the given train.csv and test.csv datasets, you can follow these steps:\nData Preprocessing\nLoad the train.csv dataset and perform necessary data cleaning and preprocessing steps such as handling missing values, encoding categorical variables, and splitting the data into features (X_train) and target (y_train). Similarly, preprocess the test.csv dataset, ensuring that it undergoes the same preprocessing steps as the training data. However, since the ‚Äúpurchase‚Äù column is missing in the test dataset, you can exclude it from the features (X_test) and treat it as the target variable that you want to predict. Feature Selection and Engineering:\nBased on the analysis of the data and any available correlation insights, select the relevant features that have a strong impact on the target variable. Perform any feature engineering techniques such as creating new features, scaling/normalizing the data, or transforming variables if necessary. Ensure that these steps are consistently applied to both the training and test datasets. Model Selection and Training:\nChoose an appropriate machine learning algorithm for regression, such as Linear Regression, Random Forest Regression, or Gradient Boosting Regression. Split the preprocessed training data (X_train and y_train) into training and validation sets. Train your chosen model on the training data and tune hyperparameters if necessary, using techniques like cross-validation and grid search. Model Evaluation:\nEvaluate the performance of your trained model on the validation set using appropriate evaluation metrics such as mean squared error (MSE), root mean squared error (RMSE), or R-squared. This will give you an idea of how well your model is performing. Model Prediction:\nOnce you are satisfied with the model‚Äôs performance, use it to predict the ‚Äúpurchase‚Äù values for the preprocessed test dataset (X_test) that does not have the ‚Äúpurchase‚Äù column. The predicted values will serve as the predicted purchase amounts for each customer and product combination in the test dataset. CODE\nwith open('test.csv' , 'r') as f: test = pd.read_csv(f) # Instantiate the LabelEncoder le = LabelEncoder() train_ml = sales_df.copy() test_ml = test.copy() train_ml['User_ID']=le.fit_transform(train_ml['User_ID']) test_ml['User_ID']=le.fit_transform(test_ml['User_ID']) train_ml['Product_ID']=le.fit_transform(train_ml['Product_ID']) test_ml['Product_ID']=le.fit_transform(test_ml['Product_ID']) train_ml['Age']=train_ml['Age'].map({'0-17':17,'55+':60,'26-35':35, '46-50':50,'51-55':55,'36-45':45,'18-25':25}) test_ml['Age']=test_ml['Age'].map({'0-17':17,'55+':60,'26-35':35,'46-50':50,'51-55':55,'36-45':45,'18-25':25}) train_ml['Stay_In_Current_City_Years']=train_ml['Stay_In_Current_City_Years'].map({'2':2,'4+':4, '3':3,'1':1,'0':0}) test_ml['Stay_In_Current_City_Years']=test_ml['Stay_In_Current_City_Years'].map({'2':2,'4+':4, '3':3,'1':1,'0':0}) category_train_ml=train_ml.select_dtypes(include=[object]).columns le=LabelEncoder() for col in category_train_ml: train_ml[col]=le.fit_transform(train_ml[col]) categorical_test_ml=test_ml.select_dtypes(include=[object]).columns for cols in categorical_test_ml: test_ml[cols]=le.fit_transform(test_ml[cols]) train_ml.tail() OUTPUT\nUser_ID Product_ID Gender Age Occupation City_Category 550063 5883 3567 1 55 13 1\r550064 5885 3568 0 35 1 2\r550065 5886 3568 0 35 15 1\r550066 5888 3568 0 60 1 2\r550067 5889 3566 0 50 0 1\rStay_In_Current_City_Years Marital_Status Product_Category_1 550063 1 1 20\r550064 3 0 20\r550065 4 1 20\r550066 2 0 20\r550067 4 1 20\rProduct_Category_2 Product_Category_3 Purchase 550063 9.842329 12.668243 368 550064 9.842329 12.668243 371 550065 9.842329 12.668243 137 550066 9.842329 12.668243 365 550067 9.842329 12.668243 490 Here‚Äôs what we did above, as you can guess from the output.\nThe test.csv file is being read using pandas‚Äô read_csv function and stored in a variable called test. Previously we worked only with train.csv but since we are now building the actual ML model, we will need the test data set as well. An instance of the LabelEncoder class is being created and stored in a variable called le. The sales_df dataframe is being copied into two new dataframes called train_ml and test_ml. The User_ID and Product_ID columns of both train_ml and test_ml is being encoded using the fit_transform method of le. The Age column of both train_ml and test_ml is being mapped to new values using a dictionary. The Stay_In_Current_City_Years column of both train_ml and test_ml is being mapped to new values using a dictionary. All categorical columns of train_ml is being encoded using the fit_transform method of le. All categorical columns of test_ml is being encoded using the fit_transform method of le. To improve Feature Selection, the following steps will prove beneficial.\nLoad and preprocess your dataset. Split the dataset into input features (X) and the target variable (y). Import the necessary libraries for feature selection. Apply one or more feature selection techniques to evaluate the importance of each feature. Select the top k features based on their importance scores or other criteria. Subset your dataset to include only the selected features. Train your model using the subset of selected features. Evaluate the performance of your model using appropriate metrics. Improving feature selection involves identifying the most relevant and informative features for your prediction task. Here are some approaches to improve feature selection:\nUnivariate Feature Selection: Use statistical tests or metrics to evaluate the relationship between each feature and the target variable independently. Select the features with the highest scores or p-values as the most relevant.\nRecursive Feature Elimination: Train a model using all features and recursively eliminate the least important features based on their coefficients or feature importances. This iterative process helps identify the subset of features that contribute the most to the model‚Äôs performance.\nFeature Importance from Tree-based Models: Train tree-based models such as Random Forest or XGBoost and extract the feature importances. Select the features with the highest importances as they have a greater impact on the model‚Äôs predictions.\nRegularization Techniques: Use regularization techniques like L1 (Lasso) or L2 (Ridge) regularization to penalize less important features and encourage sparsity. These techniques can help automatically select the most informative features.\nDomain Knowledge and Feature Engineering: Leverage your domain knowledge to engineer new features or transform existing ones that may provide more relevant information for the prediction task. Feature engineering can significantly improve the performance of your model.\nDimensionality Reduction Techniques: Apply dimensionality reduction techniques like Principal Component Analysis (PCA) or Singular Value Decomposition (SVD) to reduce the dimensionality of the feature space while retaining most of the important information. This can help eliminate redundant or less informative features.\nRegular Monitoring and Iterative Improvement: Continuously monitor the performance of your model and iterate on feature selection. Experiment with different combinations of features, feature transformations, and feature engineering techniques to find the most effective set of features.\nBuilding the ML Model After completing the data preprocessing, analysis, visualization, and label encoding, we are now ready to build a machine learning model to predict the ‚ÄúPurchase‚Äù value for company ABC. In this case, we are dealing with a regression problem since we want to estimate a continuous numerical value.\nAmong the various regression models available, one of the top models provided by the scikit-learn library is the Linear Regression model. Linear Regression is a popular and widely used regression technique that assumes a linear relationship between the input features and the target variable. It aims to find the best-fit line that minimizes the difference between the actual and predicted values.\nThe Linear Regression model in scikit-learn provides various functionalities, including:\nHandling multiple input features and calculating their coefficients. Performing feature scaling to standardize the input features. Handling categorical variables using techniques like one-hot encoding or label encoding. Evaluating the model‚Äôs performance using various metrics such as mean squared error (MSE), root mean squared error (RMSE), mean absolute error (MAE), and coefficient of determination (R-squared). These metrics help us assess how well the model fits the data and how accurate its predictions are. To select the best regression model for our task, we will train and evaluate multiple regression models, such as Linear Regression, Decision Tree Regression, Random Forest Regression, and XGBoost Regression and evaluate their performance to see which yields the highest accuracy.\nBefore moving on to the coding phase, let us first identify the metrics to identify the best performing model.\nR2 score, also known as the coefficient of determination, measures the proportion of the variance in the target variable that can be explained by the model. It indicates how well the model fits the data, with a higher value indicating a better fit. MSE calculates the average squared difference between the actual and predicted values, while RMSE is the square root of MSE, providing a more interpretable metric in the original unit of the target variable. Lower MSE and RMSE values indicate better accuracy and less error in the predictions. LINEAR REGRESSION\nIt is a simple and widely used regression algorithm that assumes a linear relationship between the input features and the target variable. It calculates the coefficients for each feature to fit a best-fit line to the data. It is easy to interpret and provides insights into the impact of each feature on the target variable.\nCODE\nx= train_ml.drop(['Purchase'],axis=1) y= train_ml['Purchase'] test_x=test_ml train_x,val_x,train_y,val_y=train_test_split(x,y,test_size=0.2,random_state=42,shuffle=True) # LINEAR REGRESSION lr=LinearRegression() lr_model=lr.fit(train_x,train_y) pred_lr=lr_model.predict(val_x) mse = mean_squared_error(pred_lr, val_y) print(\"Linear REG Mean Square Error: \", mse) rmse_lr = np.sqrt(mean_squared_error(val_y, pred_lr)) print(\"Linear REG Root Mean Square Error: \", rmse_lr) features_lr = x.columns coeff_lr = lr_model.coef_ coefficients_lr = pd.Series(lr_model.coef_, features_lr) plt.figure(figsize=(10, 6)) sns.barplot(x=coeff_lr, y=features_lr, palette=\"Blues_r\") plt.title(\"Linear Regression Coefficients\", fontsize=16) plt.xlabel(\"Coefficient\", fontsize=12) plt.ylabel(\"Feature\", fontsize=12) plt.xticks(fontsize=10) plt.yticks(fontsize=10) plt.tight_layout() plt.show() OUTPUT\nLinear REG Mean Square Error: 21708175.443769183\rLinear REG Root Mean Square Error: 4659.203305691777 XGBOOST REGRESSION\nIt is an optimized gradient boosting framework that excels in handling structured data. It is an ensemble model that combines multiple weak learners (decision trees) to make accurate predictions. XGBoost Regression is specifically designed for regression tasks and provides excellent performance and flexibility. It handles missing values, supports regularization techniques, and offers advanced features like early stopping to prevent overfitting.\nCODE\n# XGBOOST REGRESSOR XGBoost_Regression = XGBRegressor(learning_rate=1.0, max_depth=6, min_child_weight=40, seed=0) XGBoost_Regression.fit(train_x, train_y) pred_xgb = XGBoost_Regression.predict(val_x) rmse_xgb = np.sqrt(mean_squared_error(pred_xgb, val_y)) print(\"RMSE for XGBoost Regressor:\", rmse_xgb) OUTPUT\nRMSE for XGBoost Regressor: 2591.1169777068635 In the context of XGBoost, n_estimators is a hyperparameter that represents the number of decision trees to be built in the XGBoost ensemble. Each decision tree is trained sequentially, and the final prediction is obtained by aggregating the predictions of all the trees.\nIncreasing the value of n_estimators can improve the model‚Äôs performance up to a certain point. More trees allow the model to capture more complex patterns and relationships in the data, potentially leading to better predictive performance. However, using a very large value for n_estimators can also increase the risk of overfitting the training data and may result in longer training times.\nIt is common to tune the n_estimators hyperparameter during the model selection and evaluation process. This can be done using techniques such as cross-validation or grid search, where different values of n_estimators are tested to find the optimal value that balances model performance and computational efficiency. We are setting n_estimators=100 means that the XGBoost model will be trained using 100 decision trees in the ensemble.\nRANDOM FOREST REGRESSION\nRandom Forest Regression is an ensemble model that builds a multitude of decision trees and combines their predictions to obtain a more accurate and robust result. It addresses overfitting and is effective in handling high-dimensional datasets. It provides feature importance scores to identify the most influential features.\nCODE\n# RANDOM FOREST REGRESSOR RandomForest_reg=RandomForestRegressor(max_depth=2, random_state=0) RandomForest_reg.fit(train_x,train_y) RandomForest_reg=RandomForest_reg.predict(val_x) rmse=np.sqrt(mean_squared_error(RandomForest_reg,val_y)) print(\"RMSE for Random Forest:\",rmse) OUTPUT\nRMSE for Random Forest: 4163.747031944405 ADA BOOST REGRESSION\nAdaBoost Regression is an ensemble model that iteratively improves performance by focusing on the previously misclassified instances. It combines weak learners to create a strong learner, making it suitable for regression tasks. It adapts to the data and assigns higher weights to harder-to-predict instances.\nCODE\n# ADA BOOST REGRESSOR ADBBoost_Regression=AdaBoostRegressor(n_estimators=100,random_state=0) ADBBoost_Regression.fit(train_x,train_y) pred_adb=ADBBoost_Regression.predict(val_x) rmse=np.sqrt(mean_squared_error(pred_adb,val_y)) print(\"RMSE for Adaboost Regressor:\",rmse) OUTPUT\nRMSE for Adaboost Regressor: 3595.007906514239 GRADIENT BOOST REGRESSION\nGradient Boosting Regression is another ensemble technique that combines weak learners (decision trees) in a sequential manner. It optimizes a loss function by fitting the subsequent models to the residual errors of the previous models. It is a powerful algorithm that achieves high accuracy by minimizing the loss iteratively.\nCODE\n# GRADIENT BOOSTING REGRESSOR GradientBoosting_Regression=GradientBoostingRegressor(n_estimators=100, learning_rate=1.0, random_state=0) GradientBoosting_Regression.fit(train_x,train_y) GradientBoostingRegressor(learning_rate=1.0, random_state=0) gbr_predicition=GradientBoosting_Regression.predict(val_x) rmse=np.sqrt(mean_squared_error(gbr_predicition,val_y)) print(\"RMSE for Gradient Boosting Regressor:\",rmse) OUTPUT\nRMSE for Gradient Boosting Regressor: 2756.5231625627925 CODE\nRMSE for Gradient Boosting Regressor: 2756.5231625627925 from sklearn.metrics import r2_score r2 = r2_score(val_y, gbr_predicition) print(\"R2 Score for Gradient Boosting Regressor:\", r2) OUTPUT\nR2 Score for Gradient Boosting Regressor: 0.6975895276400425 Feature Importance is a technique used to determine the relevance or contribution of each feature in the prediction task. It helps identify the columns that are most useful in predicting the target variable. Techniques like permutation importance, Gini importance, or feature importance scores provided by ensemble models like Random Forest or XGBoost library.\nCODE\n# Plot the feature importance features = x.columns importances = GradientBoosting_Regression.feature_importances_ indices = np.argsort(importances) plt.figure(figsize=(10, 6)) plt.title('Feature Importances', fontsize=16) plt.barh(range(len(indices)), importances[indices], color='#4287f5', edgecolor='black') plt.yticks(range(len(indices)), [features[i] **for** i **in** indices], fontsize=10) plt.xlabel('Relative Importance', fontsize=12) plt.show() OUTPUT\nYou can explore other popular and well-known regression techniques to boost accuracy even more. Just note that these are CPU-intensive and training time can go from several minutes to several hours depending on the scale of the dataset.\nSupport Vector Regression (SVR): SVR is a powerful algorithm for regression tasks that can handle both linear and non-linear relationships. It uses support vectors to capture the important patterns in the data.\nNeural Networks: Deep learning models, such as Multilayer Perceptron (MLP) or Recurrent Neural Networks (RNN), can be effective for regression tasks when you have large amounts of data and complex relationships.\nRidge Regression: Ridge regression is a linear regression technique that incorporates regularization to prevent overfitting and handle multicollinearity. It can be useful when you have many features.\nTo improve the model‚Äôs performance, you can try the following techniques:\nFeature Engineering: Create new features or transform existing features to provide more meaningful information to the model. For example, you could combine related features, create interaction terms, or apply mathematical transformations to certain variables. Include More Relevant Features: Explore other features that may have a significant impact on the target variable. Consider adding additional features based on domain knowledge or further analysis of the data. Remove Irrelevant Features: Identify and remove features that do not contribute much to the prediction task. These features may have low correlation with the target variable or exhibit multicollinearity with other features. Polynomial Regression: Consider using polynomial regression to capture non-linear relationships between the features and the target variable. This can be achieved by creating polynomial features or using polynomial regression algorithms. Regularization Techniques: Apply regularization techniques like Ridge Regression or Lasso Regression to prevent overfitting and improve the model‚Äôs generalization ability. Regularization helps in reducing the impact of irrelevant or noisy features. Ensemble Methods: Explore ensemble methods such as Random Forests or Gradient Boosting. These techniques combine multiple models to make more accurate predictions and can handle complex relationships between features and the target variable. Hyperparameter Tuning: Optimize the hyperparameters of your chosen algorithm using techniques like grid search or random search. This involves systematically trying different combinations of hyperparameter values to find the best configuration for your model. Cross-Validation: Use cross-validation techniques to better estimate the model‚Äôs performance and reduce overfitting. This helps ensure that the model‚Äôs performance is not dependent on a specific train-test split. Collect More Data: If possible, collect more data to increase the diversity and quantity of samples available for training. More data can often improve the model‚Äôs accuracy and generalization. Making the Actual Predictions CODE\n# save the model to disk import pickle filename = 'finalized_model.sav' pickle.dump(GradientBoosting_Regression, open(filename, 'wb')) # load the model from disk with open('finalized_model.sav', 'rb') as file: ``ML_MODEL = pickle.load(file) print(ML_MODEL) OUTPUT\nGradientBoostingRegressor(learning_rate=1.0, random_state=0) CODE\nXGBoost_Regression.fit(x, y) predict_final = XGBoost_Regression.predict(test_x) # make a predictions dataframe predictions = pd.DataFrame() predictions['Purchase'] = predict_final predictions.to_csv('sales_prediction.csv', index=False) OUTPUT\nPurchase 12624.37 13786.86 3508.918 The sales prediction CSV has the above format. To examine the projected output for each user, copy and paste this column into the original train.csv and test.csv files. The rows are in the same order as they were in the original files.\nConclusion This exercise explored the analysis of customer purchasing habits and predicting their spending on products. By understanding customer behavior and preferences, businesses can offer personalized offers and improve their customer targeting strategies.\nLet‚Äôs recap the sales analysis and prediction:\nUnderstanding customer purchasing habits and analyzing customer behavior is essential for businesses to personalize their offerings and enhance customer satisfaction. Through data preprocessing and exploratory data analysis, you gained valuable insights into the dataset, uncovering patterns and relationships.\nAddressing outliers and visualizing the data through various techniques provided further understanding of the purchasing patterns and correlations between variables. Label encoding and feature selection helped prepare the data for model building by selecting relevant features that contribute to predicting customer spending.\nImplementing regression models such as Linear Regression, XGBoost Regression, RandomForest Regression, ADA Boost Regression, and Gradient Boost Regression allowed you to leverage customer characteristics and product categories to predict spending accurately.\nEvaluating the model‚Äôs performance using metrics like R2 score, mean square error (MSE), and root mean square error (RMSE) revealed the accuracy and effectiveness of the models. The Gradient Boost Regression model achieved an R2 score of approximately 0.70, indicating that around 70% of the variation in customer spending was explained by the model.\nIn conclusion, by analyzing customer purchasing habits and building predictive models, you gained valuable insights for tailored marketing strategies and business decision-making. Continuous refinement and improvement of the models, along with techniques like feature engineering, hyperparameter tuning, ensemble methods, cross-validation, and handling imbalanced data, can further enhance the accuracy and performance of the models. These insights and models contribute to maximizing customer satisfaction, optimizing business profitability, and driving success in customer purchasing analysis.\n",
  "wordCount" : "6539",
  "inLanguage": "en",
  "image":"https://atharvashah.netlify.app/blog/sales-data-analysis/cover.webp","datePublished": "2023-06-14T01:18:34+05:30",
  "dateModified": "2023-06-14T01:18:34+05:30",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://atharvashah.netlify.app/posts/tech/sales-data-analysis/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Atharva Shah",
    "logo": {
      "@type": "ImageObject",
      "url": "https://atharvashah.netlify.app/favicon.ico"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    
    
    <div id="scroll-progress"></div>
    <nav class="nav">
        <div class="logo">
            <a href="https://atharvashah.netlify.app/" accesskey="h" title="Atharva Shah (Alt + H)">
            <img src="https://atharvashah.netlify.app/profile/header_button.webp" alt="logo" aria-label="logo"
                 height="30">Atharva Shah</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://atharvashah.netlify.app/posts/tech/" title="üíªTech">
                <span>üíªTech</span>
                </a>
            </li>
            <li>
                <a href="https://atharvashah.netlify.app/posts/personal/" title="üçøPersonal">
                <span>üçøPersonal</span>
                </a>
            </li>
            <li>
                <a href="https://atharvashah.netlify.app/about/" title="üë®üèªAbout">
                <span>üë®üèªAbout</span>
                </a>
            </li>
            <li>
                <a href="https://atharvashah.netlify.app/tags/" title="üè∑Ô∏èTags">
                <span>üè∑Ô∏èTags</span>
                </a>
            </li>
            <li>
                <a href="https://atharvashah.netlify.app/archives/" title="üìúArchive">
                <span>üìúArchive</span>
                </a>
            </li>
            <li>
                <a href="https://atharvashah.netlify.app/search/" title="üîçSearch (Alt &#43; /)" accesskey=/>
                <span>üîçSearch</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
    <main class="main">

<article class="post-single">
    <header class="post-header">
    <div class="breadcrumbs"><a href="https://atharvashah.netlify.app/">Home</a>&nbsp;¬ª&nbsp;<a href="https://atharvashah.netlify.app/posts/">üìö All Posts</a>&nbsp;¬ª&nbsp;<a href="https://atharvashah.netlify.app/posts/tech/">üíªTech</a></div>
    <h1 class="post-title">
      Sales Data Analysis with Python
    </h1>
    <div class="post-description">
      This tutorial explores data preprocessing, exploratory data analysis, feature engineering, and model building for sales data analysis.
    </div>
    <div class="post-meta">

<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }

    .parent-post-meta {
        display: flex;
        flex-wrap: wrap;
        opacity: 0.8;
    }
</style>

<span class="parent-post-meta">
    <span id="post_meta_style_1">
        <span class="fa fa-calendar-check-o"></span>
        <span>June 14, 2023
            &nbsp;&nbsp;
        </span>
    </span>
    
    
    
    
    
    
    
    <span id="post_meta_style_3">
        <span class="fa fa-file-word-o"></span>
        <span>6539 words
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_4">
        <span class="fa fa-clock-o"></span>
        <span>31 mins
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_6">
        <span class="fa fa-tags" style="opacity: 0.8"></span>
        <span>
            <span class="post-tags-meta">
                <a href="https://atharvashah.netlify.app/tags/python/" style="color: var(--secondary)!important;">python</a>
                &nbsp;<a href="https://atharvashah.netlify.app/tags/tutorial/" style="color: var(--secondary)!important;">tutorial</a>
            </span>
        </span>
    </span>
</span>

</div>
  </header> 
<figure class="entry-cover"><img loading="lazy" src="https://atharvashah.netlify.app/blog/sales-data-analysis/cover.webp" alt="EDA, Preprocessing, Feature Engineering, Model Building with Python">
        <p>EDA, Preprocessing, Feature Engineering, Model Building with Python</p>
</figure><aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#data-preprocessing" aria-label="Data Preprocessing">Data Preprocessing</a></li>
                <li>
                    <a href="#descriptive-statistics" aria-label="Descriptive Statistics">Descriptive Statistics</a></li>
                <li>
                    <a href="#outlier-analysis" aria-label="Outlier Analysis">Outlier Analysis</a></li>
                <li>
                    <a href="#a-namexde973c9a92a75ef55aa425326f5364dcc8fad3aaconvert-categorical-and-range-variables-to-integers" aria-label="Convert Categorical and Range Variables to Integers"><!-- raw HTML omitted --><!-- raw HTML omitted -->Convert Categorical and Range Variables to Integers</a></li>
                <li>
                    <a href="#purchase-distribution-and-correlation-matrix" aria-label="Purchase Distribution and Correlation Matrix">Purchase Distribution and Correlation Matrix</a></li>
                <li>
                    <a href="#data-visualization" aria-label="Data Visualization">Data Visualization</a></li>
                <li>
                    <a href="#label-encoding-and-feature-selection" aria-label="Label Encoding and Feature Selection">Label Encoding and Feature Selection</a></li>
                <li>
                    <a href="#building-the-ml-model" aria-label="Building the ML Model">Building the ML Model</a></li>
                <li>
                    <a href="#making-the-actual-predictions" aria-label="Making the Actual Predictions">Making the Actual Predictions</a></li>
                <li>
                    <a href="#conclusion" aria-label="Conclusion">Conclusion</a>
                </li>
            </ul>
        </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        
        activeElement = elements[0];
        const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
        document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
    }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        if (elements) {
            activeElement = Array.from(elements).find((element) => {
                if ((getOffsetTop(element) - window.pageYOffset) > 0 &&
                    (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                    return element;
                }
            }) || activeElement

            elements.forEach(element => {
                const id = encodeURI(element.getAttribute('id')).toLowerCase();
                if (element === activeElement){
                    document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
                } else {
                    document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
                }
            })
        }
    }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;
    }
</script>
  <div class="post-content"><h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>ABC Private Limited, a retail company, aims to gain valuable insights into their customers&rsquo; purchasing habits. By analyzing the provided summary of high-volume product purchase history, which includes customer demographics and product details, ABC can uncover patterns and trends in customer spending across different product categories.</p>
<p>We will explore the analysis step-by-step using the <strong>test.csv</strong> and <strong>train.csv</strong> files. Through a combination of theoretical explanations and practical demonstrations, we will delve into data preprocessing, exploratory data analysis, feature engineering, and model building. We will be utilizing the power of Python for conducting Exploratory Data Analysis, performing data visualization, and training machine learning models. It is recommended to follow along using a Jupyter Notebook, as it provides an interactive and user-friendly environment for code execution and documentation.</p>
<p>To proceed with the analysis, we will make use of various Python libraries, including:</p>
<ul>
<li><strong>pandas</strong>: for data manipulation and analysis</li>
<li><strong>matplotlib</strong>: data visualization</li>
<li><strong>scikit</strong>-<strong>learn</strong>: evaluating and training machine learning models</li>
<li><strong>numpy</strong>: numerical computations</li>
<li><strong>seaborn</strong>: enhanced data visualization</li>
<li><strong>xgboost</strong>: implementing gradient boosting algorithms</li>
<li><strong>sklearn</strong>: additional machine learning tools and utilities</li>
</ul>
<p>üí° Make sure to have these packages installed in your Python environment. You can install them using pip or conda by running the following commands:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cmd" data-lang="cmd"><span style="display:flex;"><span>pip install pandas matplotlib scikit-learn numpy seaborn xgboost sklearn
</span></span></code></pre></div><h2 id="data-preprocessing">Data Preprocessing<a hidden class="anchor" aria-hidden="true" href="#data-preprocessing">#</a></h2>
<p>Let‚Äôs begin with the exercise by importing the necessary modules. You have already installed these in the previous section.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> seaborn <span style="color:#66d9ef">as</span> sns
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn <span style="color:#f92672">import</span> metrics
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.preprocessing <span style="color:#f92672">import</span> LabelEncoder,minmax_scale,scale
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.linear_model <span style="color:#f92672">import</span> LinearRegression
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.preprocessing <span style="color:#f92672">import</span> StandardScaler
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.tree <span style="color:#f92672">import</span> DecisionTreeRegressor
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.ensemble <span style="color:#f92672">import</span> RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> mean_absolute_error,mean_squared_error
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> xgboost.sklearn <span style="color:#f92672">import</span> XGBRegressor
</span></span></code></pre></div><h2 id="descriptive-statistics">Descriptive Statistics<a hidden class="anchor" aria-hidden="true" href="#descriptive-statistics">#</a></h2>
<p><strong>CODE</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># STEP 1: LOAD THE DATA</span>
</span></span><span style="display:flex;"><span>sales_df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_csv(<span style="color:#e6db74">&#39;train.csv&#39;</span>, encoding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;utf-8&#39;</span>, sep<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;,&#39;</span>, header<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># print the shape of the dataset [rows, columns]</span>
</span></span><span style="display:flex;"><span>rows, columns <span style="color:#f92672">=</span> sales_df<span style="color:#f92672">.</span>shape
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;Shape = </span><span style="color:#e6db74">{</span>rows<span style="color:#e6db74">}</span><span style="color:#e6db74"> rows x</span><span style="color:#e6db74">{</span>columns<span style="color:#e6db74">}</span><span style="color:#e6db74"> columns&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># print the datatype of each column</span>
</span></span><span style="display:flex;"><span>sales_df<span style="color:#f92672">.</span>dtypes
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># print the datatype of each column</span>
</span></span><span style="display:flex;"><span>sales_df<span style="color:#f92672">.</span>head()
</span></span></code></pre></div><p><strong>OUTPUT</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>Shape <span style="color:#f92672">=</span> <span style="color:#ae81ff">550068</span> rows x <span style="color:#ae81ff">12</span> columns
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>User_ID                         int64
</span></span><span style="display:flex;"><span>Product_ID                     object
</span></span><span style="display:flex;"><span>Gender                         object
</span></span><span style="display:flex;"><span>Age                            object
</span></span><span style="display:flex;"><span>Occupation                      int64
</span></span><span style="display:flex;"><span>City_Category                  object
</span></span><span style="display:flex;"><span>Stay_In_Current_City_Years     object
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Marital_Status                  int64
</span></span><span style="display:flex;"><span>Product_Category_1              int64
</span></span><span style="display:flex;"><span>Product_Category_2            float64
</span></span><span style="display:flex;"><span>Product_Category_3            float64
</span></span><span style="display:flex;"><span>Purchase                        int64
</span></span><span style="display:flex;"><span>dtype: object
</span></span></code></pre></div><p>To start off we did the following:-</p>
<ul>
<li>Loaded the data from the &ldquo;train.csv&rdquo; file into the pandas DataFrame called sales_df and used pd.read_csv() function with &lsquo;utf-8&rsquo; encoding and &lsquo;,&rsquo; separator to read the CSV file.</li>
<li>Set header=0 to indicate that the first row contains column names.</li>
<li>Printed the shape of the DataFrame to show the number of rows and columns.</li>
<li>Retrieved the number of rows and columns using sales_df.shape and stored them in rows and columns variables.</li>
<li>Printed the information about the dataset in a formatted string.</li>
<li>Checked the data type of each column using sales_df.dtypes to understand the types of features in the dataset.</li>
<li>Identified data types such as int64 for integers and object for strings or categorical variables.</li>
<li>Printed the first five rows to get an overview of the nature of columns and the sales data.</li>
</ul>
<p><strong>CODE</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># Check descriptive statistics of the dataset</span>
</span></span><span style="display:flex;"><span>sales_df<span style="color:#f92672">.</span>describe()
</span></span></code></pre></div><p><strong>OUTPUT</strong></p>
<pre tabindex="0"><code>User_ID     Occupation  Marital_Status  Product_Category_1  
count  5.500680e+05  550068.000000   550068.000000       550068.000000
mean   1.003029e+06       8.076707        0.409653            5.404270
std    1.727592e+03       6.522660        0.491770            3.936211
min    1.000001e+06       0.000000        0.000000            1.000000
25%    1.001516e+06       2.000000        0.000000            1.000000
50%    1.003077e+06       7.000000        0.000000            5.000000
75%    1.004478e+06      14.000000        1.000000            8.000000
max    1.006040e+06      20.000000        1.000000           20.000000

Product_Category_2  Product_Category_3       Purchase  
count       376430.000000       166821.000000  550068.000000  
mean             9.842329           12.668243    9263.968713  
std              5.086590            4.125338    5023.065394  
min              2.000000            3.000000      12.000000  
25%              5.000000            9.000000    5823.000000  
50%              9.000000           14.000000    8047.000000  
75%             15.000000           16.000000   12054.000000  
max             18.000000           18.000000   23961.000000  
</code></pre><p><strong>CODE</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>categorical<span style="color:#f92672">=</span>sales_df<span style="color:#f92672">.</span>select_dtypes(include<span style="color:#f92672">=</span>[object])
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Categorical columns:&#34;</span>,categorical<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>numerical<span style="color:#f92672">=</span>sales_df<span style="color:#f92672">.</span>select_dtypes(include<span style="color:#f92672">=</span>[np<span style="color:#f92672">.</span>float64,np<span style="color:#f92672">.</span>int64])
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Numerical columns:&#34;</span>,numerical<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># check for missing values in the data</span>
</span></span><span style="display:flex;"><span>sales_df<span style="color:#f92672">.</span>isnull()<span style="color:#f92672">.</span>sum()
</span></span></code></pre></div><p><strong>OUTPUT</strong></p>
<pre tabindex="0"><code>Categorical columns: 5
Numerical columns: 7

User_ID                            0
Product_ID                         0
Gender                             0
Age                                0
Occupation                         0
City_Category                      0
Stay_In_Current_City_Years         0
Marital_Status                     0
Product_Category_1                 0
Product_Category_2            173638
Product_Category_3            383247
Purchase                           0
dtype: int64
</code></pre><p>The takeaway from this so far is:-</p>
<ul>
<li>Identified 5 categorical columns and 7 numerical columns in the dataset.</li>
<li>Checked for missing values using sales_df.isnull().sum().</li>
<li>Found that Product_Category_2 has 173,638 missing values and Product_Category_3 has 383,247 missing values.</li>
<li>No missing values were found in other columns.</li>
</ul>
<p><strong>üí° Missing values can alter the process because they introduce uncertainty and can affect the statistical properties of the data. Missing data can lead to biased analysis, inaccurate results, and reduced performance of machine learning models. Therefore, it is crucial to handle missing values appropriately to ensure the integrity and reliability of the analysis.</strong></p>
<p>Ways to handle missing values:</p>
<ol>
<li>Filling them with a suitable value, such as the mean, median, or mode of the column. This approach is used here, where the missing values in &lsquo;Product_Category_2&rsquo; and &lsquo;Product_Category_3&rsquo; are filled with their respective means.</li>
<li>Remove the rows or columns with missing values, but this can lead to data loss and should be done cautiously.</li>
<li>Depending on the context and the significance of the missing values, other advanced techniques like imputation methods (e.g., regression imputation, KNN imputation) or using machine learning models to predict missing values can be employed.</li>
</ol>
<p>As seen above,  Product_Category_2 and Product_Category_3 have several values missing, so let‚Äôs deal with it below.</p>
<p><strong>CODE</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># Fill missing values with mean</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>sales_df[<span style="color:#e6db74">&#39;Product_Category_2&#39;</span>]<span style="color:#f92672">=</span>sales_df[<span style="color:#e6db74">&#39;Product_Category_2&#39;</span>]<span style="color:#f92672">.</span>fillna(sales_df[<span style="color:#e6db74">&#39;Product_Category_2&#39;</span>]<span style="color:#f92672">.</span>mean())
</span></span><span style="display:flex;"><span>sales_df[<span style="color:#e6db74">&#39;Product_Category_3&#39;</span>]<span style="color:#f92672">=</span>sales_df[<span style="color:#e6db74">&#39;Product_Category_3&#39;</span>]<span style="color:#f92672">.</span>fillna(sales_df[<span style="color:#e6db74">&#39;Product_Category_3&#39;</span>]<span style="color:#f92672">.</span>mean())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Check for missing values in the data</span>
</span></span><span style="display:flex;"><span>sales_df<span style="color:#f92672">.</span>isnull()<span style="color:#f92672">.</span>sum()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># see which columns have non-repeating vals</span>
</span></span><span style="display:flex;"><span>sales_df<span style="color:#f92672">.</span>nunique()
</span></span></code></pre></div><p><strong>OUTPUT</strong></p>
<pre tabindex="0"><code>User_ID                       0
Product_ID                    0
Gender                        0
Age                           0
Occupation                    0
City_Category                 0
Stay_In_Current_City_Years    0
Marital_Status                0
Product_Category_1            0
Product_Category_2            0
Product_Category_3            0
Purchase                      0
dtype: int64

User_ID                        5891
Product_ID                     3631
Gender                            2
Age                               7
Occupation                       21
City_Category                     3
Stay_In_Current_City_Years        5
Marital_Status                    2
Product_Category_1               20
Product_Category_2               18
Product_Category_3               16
Purchase                      18105
dtype: int64
</code></pre><p><strong>Here, to deal with unstructured data and missing values we did the following:-</strong></p>
<ul>
<li>Missing values in the columns &lsquo;Product_Category_2&rsquo; and &lsquo;Product_Category_3&rsquo; are filled with the mean value of each respective column using the fillna() function.</li>
<li>Checked for missing values again using sales_df.isnull().sum() and confirmed that there are no missing values in any column.</li>
<li>Utilized sales df.nunique() method to determine the number of distinct values in each column. Understanding the variety and cardinality of each characteristic is aided by this.</li>
</ul>
<h2 id="outlier-analysis">Outlier Analysis<a hidden class="anchor" aria-hidden="true" href="#outlier-analysis">#</a></h2>
<p><strong>Outliers</strong> are data points that deviate significantly from most of the data in a dataset. They can be unusually high or low values that are distant from the central tendency of the distribution. Outliers can occur due to various reasons, such as measurement errors, data entry mistakes, or genuine extreme observations.</p>
<p>Outliers are harmful because they can distort the statistical analysis and modeling process. They can have a disproportionate impact on statistical measures such as mean and standard deviation, leading to biased estimates. Outliers can also affect the performance of machine learning models by introducing noise and influencing the model&rsquo;s decision boundaries.</p>
<p><strong>Boxplots</strong> are a graphical representation that displays the distribution of a dataset, including information about outliers. In a boxplot, the box represents the interquartile range (IQR), the line inside the box represents the median, and the whiskers extend to the minimum and maximum values within a certain range.</p>
<p><strong>CODE</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># make a boxplot to show outliers, only for numerical columns</span>
</span></span><span style="display:flex;"><span>columns <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;Purchase&#39;</span>, <span style="color:#e6db74">&#39;Product_Category_1&#39;</span>, <span style="color:#e6db74">&#39;Product_Category_2&#39;</span>, <span style="color:#e6db74">&#39;Occupation&#39;</span>]
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> column <span style="color:#f92672">in</span> columns:
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>figure()
</span></span><span style="display:flex;"><span>    sns<span style="color:#f92672">.</span>boxplot(data<span style="color:#f92672">=</span>sales_df[[column]])
</span></span></code></pre></div><p><strong>OUTPUT</strong></p>
<p>






<figure>
  
    <img
      sizes="100vw"
      src="/blog/sales-data-analysis/Aspose.Words.3f482350-c406-414e-b02b-91d71b09cc1b.004.webp"
      alt="A picture containing text, screenshot, number, line automatically generated"
      loading="lazy"
      >
  
  
</figure>






<figure>
  
    <img
      sizes="100vw"
      src="/blog/sales-data-analysis/Aspose.Words.3f482350-c406-414e-b02b-91d71b09cc1b.005.webp"
      alt="output-ss"
      loading="lazy"
      >
  
  
</figure></p>
<p>






<figure>
  
    <img
      sizes="100vw"
      src="/blog/sales-data-analysis/Aspose.Words.3f482350-c406-414e-b02b-91d71b09cc1b.006.webp"
      alt="A picture containing screenshot, text, rectangle, plot automatically generated"
      loading="lazy"
      >
  
  
</figure></p>
<p>






<figure>
  
    <img
      sizes="100vw"
      src="/blog/sales-data-analysis/Aspose.Words.3f482350-c406-414e-b02b-91d71b09cc1b.007.webp"
      alt="A picture containing screenshot, text, rectangle, line Description automatically generated"
      loading="lazy"
      >
  
  
</figure></p>
<p>To deal with outliers using boxplots, one approach is to identify and remove the outliers based on predetermined thresholds. This can be done by considering values that fall below the lower whisker or above the upper whisker as outliers. Alternatively, instead of removing outliers, they can be treated or transformed using techniques such as winsorization (replacing extreme values with a specified percentile value) or logarithmic transformations.</p>
<p>In the provided code, a boxplot is created for each numerical column (&lsquo;Purchase&rsquo;, &lsquo;Product_Category_1&rsquo;, &lsquo;Product_Category_2&rsquo;, &lsquo;Occupation&rsquo;) using seaborn&rsquo;s boxplot function. This allows visual identification of any outliers present in the data for these specific columns.</p>
<h2 id="a-namexde973c9a92a75ef55aa425326f5364dcc8fad3aaconvert-categorical-and-range-variables-to-integers"><!-- raw HTML omitted --><!-- raw HTML omitted -->Convert Categorical and Range Variables to Integers<a hidden class="anchor" aria-hidden="true" href="#a-namexde973c9a92a75ef55aa425326f5364dcc8fad3aaconvert-categorical-and-range-variables-to-integers">#</a></h2>
<p><strong>Categorical variables</strong> are variables that represent qualitative attributes or characteristics. They take on a limited number of distinct categories or levels. Examples of categorical variables include gender, occupation, and city category.</p>
<p><strong>Range variables</strong>, also known as continuous variables, represent quantitative attributes that can take on any value within a certain range. Examples of range variables include age, income, and purchase amount.</p>
<p>Converting categorical and range variables to integers is important for several reasons:</p>
<ol>
<li>
<p><strong>Numerical representation</strong>: Many machine learning algorithms and statistical models require numerical input. By converting categorical and range variables to integers, we can ensure that the data can be processed and analyzed correctly.</p>
</li>
<li>
<p><strong>Simplification of calculations</strong>: Numeric representation simplifies calculations and computations. It allows for arithmetic operations and comparisons, which are essential in various statistical analyses and modeling techniques.</p>
</li>
<li>
<p><strong>Standardization and scaling</strong>: Converting variables to integers can help standardize and scale the data. This is particularly important when using certain algorithms that are sensitive to the scale of the input features. Standardization can improve the performance and stability of these algorithms.</p>
</li>
<li>
<p>E<strong>ncoding categorical variables</strong>: Converting categorical variables to integers allows us to apply encoding techniques such as one-hot encoding or label encoding.</p>
</li>
</ol>
<p><strong>CODE</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>print(sales_df[<span style="color:#e6db74">&#39;Stay_In_Current_City_Years&#39;</span>]<span style="color:#f92672">.</span>value_counts())
</span></span><span style="display:flex;"><span>print(sales_df[<span style="color:#e6db74">&#39;City_Category&#39;</span>]<span style="color:#f92672">.</span>value_counts())
</span></span><span style="display:flex;"><span>print(sales_df[<span style="color:#e6db74">&#39;Age&#39;</span>]<span style="color:#f92672">.</span>value_counts())
</span></span><span style="display:flex;"><span>print(sales_df[<span style="color:#e6db74">&#39;Gender&#39;</span>])
</span></span></code></pre></div><p><strong>OUTPUT</strong></p>
<pre tabindex="0"><code>1     193821
2     101838
3      95285
4+     84726
0      74398
Name: Stay_In_Current_City_Years, dtype: int64
B    231173
C    171175
A    147720
Name: City_Category, dtype: int64
26-35    219587
36-45    110013
18-25     99660
46-50     45701
51-55     38501
55+       21504
0-17      15102
Name: Age, dtype: int64
0         F
1         F
2         F
3         F
4         M
..
550063    M
550064    F
550065    F
550066    F
550067    F
Name: Gender, Length: 550068, dtype: object
</code></pre><p><strong>CODE</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># Group by Product_Category_1 and calculate the mean purchase price</span>
</span></span><span style="display:flex;"><span>product_category_1_mean <span style="color:#f92672">=</span> sales_df[[<span style="color:#e6db74">&#39;Product_Category_1&#39;</span>, <span style="color:#e6db74">&#39;Purchase&#39;</span>]]<span style="color:#f92672">.</span>groupby([<span style="color:#e6db74">&#39;Product_Category_1&#39;</span>], as_index<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)<span style="color:#f92672">.</span>mean()<span style="color:#f92672">.</span>sort_values(by<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Purchase&#39;</span>, ascending<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>product_category_1_mean<span style="color:#f92672">.</span>columns <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;Product_Category_1&#39;</span>, <span style="color:#e6db74">&#39;Mean_Purchase_Price&#39;</span>]
</span></span><span style="display:flex;"><span>product_category_1_mean[<span style="color:#e6db74">&#39;Mean_Purchase_Price&#39;</span>] <span style="color:#f92672">=</span> product_category_1_mean[<span style="color:#e6db74">&#39;Mean_Purchase_Price&#39;</span>]<span style="color:#f92672">.</span>map(<span style="color:#e6db74">&#39;</span><span style="color:#e6db74">{:.2f}</span><span style="color:#e6db74">&#39;</span><span style="color:#f92672">.</span>format)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Group by Product_Category_2 and calculate the mean purchase price</span>
</span></span><span style="display:flex;"><span>product_category_2_mean <span style="color:#f92672">=</span> sales_df[[<span style="color:#e6db74">&#39;Product_Category_2&#39;</span>, <span style="color:#e6db74">&#39;Purchase&#39;</span>]]<span style="color:#f92672">.</span>groupby([<span style="color:#e6db74">&#39;Product_Category_2&#39;</span>], as_index<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)<span style="color:#f92672">.</span>mean()<span style="color:#f92672">.</span>sort_values(by<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Purchase&#39;</span>, ascending<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>product_category_2_mean<span style="color:#f92672">.</span>columns <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;Product_Category_2&#39;</span>, <span style="color:#e6db74">&#39;Mean_Purchase_Price&#39;</span>]
</span></span><span style="display:flex;"><span>product_category_2_mean[<span style="color:#e6db74">&#39;Mean_Purchase_Price&#39;</span>] <span style="color:#f92672">=</span> product_category_2_mean[<span style="color:#e6db74">&#39;Mean_Purchase_Price&#39;</span>]<span style="color:#f92672">.</span>map(<span style="color:#e6db74">&#39;</span><span style="color:#e6db74">{:.2f}</span><span style="color:#e6db74">&#39;</span><span style="color:#f92672">.</span>format)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Display the results without indexes</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> pd<span style="color:#f92672">.</span>option_context(<span style="color:#e6db74">&#39;display.float_format&#39;</span>, <span style="color:#e6db74">&#39;</span><span style="color:#e6db74">{:.2f}</span><span style="color:#e6db74">&#39;</span><span style="color:#f92672">.</span>format):
</span></span><span style="display:flex;"><span>    print(product_category_1_mean<span style="color:#f92672">.</span>to_string(index<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>))
</span></span><span style="display:flex;"><span>    print(product_category_2_mean<span style="color:#f92672">.</span>to_string(index<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>))
</span></span></code></pre></div><p><strong>OUTPUT</strong></p>
<pre tabindex="0"><code>Product_Category_1 Mean_Purchase_Price
10            19675.57
7            16365.69
6            15838.48
9            15537.38
15            14780.45
16            14766.04
1            13606.22
14            13141.63
2            11251.94
17            10170.76
3            10096.71
8             7498.96
5             6240.09
11             4685.27
18             2972.86
4             2329.66
12             1350.86
13              722.40
20              370.48
19               37.04
Product_Category_2 Mean_Purchase_Price
10.00            15648.73
2.00            13619.36
6.00            11503.55
3.00            11235.36
15.00            10357.08
16.00            10295.68
8.00            10273.26
4.00            10215.19
13.00             9683.35
17.00             9421.58
18.00             9352.44
5.00             9027.82
11.00             8940.58
9.84             7518.70
9.00             7277.01
14.00             7105.26
12.00             6975.47
7.00             6884.68
</code></pre><p>As demonstrated, we are performing grouping and aggregation operation on the sales data.</p>
<ol>
<li>For Product_Category_1, we group the data by this category and calculate the mean purchase price for each category. The results are sorted in descending order of purchase price.</li>
<li>Similarly, for Product_Category_2, we group the data by this category and calculate the mean purchase price for each category. The results are also sorted in descending order of purchase price.</li>
<li>The results are displayed without indexes, and the mean purchase prices are formatted to two decimal places for better readability.</li>
</ol>
<h2 id="purchase-distribution-and-correlation-matrix">Purchase Distribution and Correlation Matrix<a hidden class="anchor" aria-hidden="true" href="#purchase-distribution-and-correlation-matrix">#</a></h2>
<p>The correlation coefficients between several variables are displayed in a table called a correlation matrix. It serves as a gauge for the strength and direction of a linear relationship between two variables.Each cell in the matrix represents the correlation coefficient between two variables, ranging from -1 to 1. A positive value indicates a positive correlation, meaning that as one variable increases, the other variable tends to increase as well. A negative value indicates a negative correlation, meaning that as one variable increases, the other variable tends to decrease. A value close to 0 indicates a weak or no correlation between the variables.</p>
<p>Analyzing the correlation matrix is important because it provides insights into the relationships between variables. It helps in understanding how different features or variables are related to each other and to the target variable. By identifying strong correlations, we can determine which variables have a significant impact on the target variable and may be important for prediction or modeling purposes. Correlation analysis also helps in feature selection, as highly correlated variables may provide redundant information and can be eliminated to simplify the model and improve interpretability.</p>
<p>In machine learning models, correlation analysis helps in several ways:</p>
<ol>
<li>
<p><strong>Feature Selection</strong>: Highly correlated features can be removed to avoid multicollinearity, where multiple variables provide similar information, leading to unstable and less interpretable models.</p>
</li>
<li>
<p><strong>Feature Engineering</strong>: By analyzing correlations, new features can be created by combining or transforming existing features, which can improve the predictive power of the model.</p>
</li>
<li>
<p><strong>Model Interpretation</strong>: Understanding the correlation between variables helps in interpreting the model&rsquo;s coefficients and understanding the direction and strength of the relationships between features and the target variable.</p>
</li>
</ol>
<p><strong>CODE</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># plot the correlation matrix as a heatmap</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">10</span>))
</span></span><span style="display:flex;"><span>sns<span style="color:#f92672">.</span>heatmap(matrix, vmax<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, square<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;BuPu&#39;</span>, annot<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Correlation between different features&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><strong>OUTPUT</strong></p>
<p>






<figure>
  
    <img
      sizes="100vw"
      src="/blog/sales-data-analysis/Aspose.Words.3f482350-c406-414e-b02b-91d71b09cc1b.008.webp"
      alt="output-ss"
      loading="lazy"
      >
  
  
</figure></p>
<p>Based on the correlation heatmap, we can observe that certain variables have a stronger correlation with the &ldquo;Purchase&rdquo; column. The variables &ldquo;User_ID,&rdquo; &ldquo;Marital Status,&rdquo; and &ldquo;Product_Category_3&rdquo; show relatively weak correlation with the purchase power of the customer. Conversely, the variables &ldquo;Occupation,&rdquo; &ldquo;Product Category 1,&rdquo; and &ldquo;Product Category 2&rdquo; exhibit a significantly higher impact on the purchase behavior. It&rsquo;s important to note that this correlation analysis is based on numerical fields only, and we will be generating an even more insightful heatmap right before preparing the ML model.</p>
<h2 id="data-visualization">Data Visualization<a hidden class="anchor" aria-hidden="true" href="#data-visualization">#</a></h2>
<p><strong>The graphical depiction of data using graphs, charts, and other visual components is known as data visualisation and it is a way to visually explore and communicate patterns, relationships, and trends within the data.</strong></p>
<p>By plotting different variables against the &ldquo;Purchase&rdquo; variable, trends can be identified through visual analysis.</p>
<p><strong>CODE</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># Check the purchase distribution with respect to Gender</span>
</span></span><span style="display:flex;"><span>purchase_gender <span style="color:#f92672">=</span> sales_df<span style="color:#f92672">.</span>groupby[<span style="color:#e6db74">&#39;Gender&#39;</span>](<span style="color:#e6db74">&#39;Purchase&#39;</span>)<span style="color:#f92672">.</span>sum() <span style="color:#f92672">/</span> <span style="color:#ae81ff">1000000</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Rename 0 and 1 to Female and Male</span>
</span></span><span style="display:flex;"><span>purchase_gender<span style="color:#f92672">.</span>index <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;Female&#39;</span>, <span style="color:#e6db74">&#39;Male&#39;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Plot the purchase distribution by Gender</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>bar(purchase_gender<span style="color:#f92672">.</span>index, purchase_gender, color<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;#F8766D&#39;</span>, <span style="color:#e6db74">&#39;#00BFC4&#39;</span>] )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Add a title</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Purchase Distribution by Gender&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Customize the x-axis and y-axis labels</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Gender&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;Purchase (in millions)&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Display the plot</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><strong>OUTPUT</strong></p>
<p>






<figure>
  
    <img
      sizes="100vw"
      src="/blog/sales-data-analysis/Aspose.Words.3f482350-c406-414e-b02b-91d71b09cc1b.009.webp"
      alt="A picture containing text, screenshot, diagram, plot Description automatically generated"
      loading="lazy"
      >
  
  
</figure></p>
<p>Based on the data provided in train.csv, males have purchased significantly more than females, almost 3x more. This would lead us to the conclusion that either the data set is skewed, or men are willing to spend much more on products by company ABC. This also speaks about purchase power parity.</p>
<p><strong>CODE</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># Check the purchase distribution with respect to Age</span>
</span></span><span style="display:flex;"><span>purchase_age <span style="color:#f92672">=</span> sales_df<span style="color:#f92672">.</span>groupby[<span style="color:#e6db74">&#39;Age&#39;</span>](<span style="color:#e6db74">&#39;Purchase&#39;</span>)<span style="color:#f92672">.</span>sum() <span style="color:#f92672">/</span> <span style="color:#ae81ff">1000000</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Assign the modified age labels as the index</span>
</span></span><span style="display:flex;"><span>purchase_age<span style="color:#f92672">.</span>index <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;0-17&#39;</span>, <span style="color:#e6db74">&#39;18-25&#39;</span>, <span style="color:#e6db74">&#39;26-35&#39;</span>, <span style="color:#e6db74">&#39;36-45&#39;</span>, <span style="color:#e6db74">&#39;46-50&#39;</span>, <span style="color:#e6db74">&#39;51-55&#39;</span>, <span style="color:#e6db74">&#39;55+&#39;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Plot the purchase distribution by Age</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>bar(purchase_age<span style="color:#f92672">.</span>index, purchase_age, color<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;#4C72B0&#39;</span>, <span style="color:#e6db74">&#39;#55A868&#39;</span>, <span style="color:#e6db74">&#39;#C44E52&#39;</span>, <span style="color:#e6db74">&#39;#8172B2&#39;</span>, <span style="color:#e6db74">&#39;#CCB974&#39;</span>, <span style="color:#e6db74">&#39;#64B5CD&#39;</span>, <span style="color:#e6db74">&#39;#4C4C4C&#39;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Add a title</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Purchase Distribution by Age&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Customize the x-axis and y-axis labels</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Age&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;Purchase (in millions)&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Display the plot</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><strong>OUTPUT</strong></p>
<p>






<figure>
  
    <img
      sizes="100vw"
      src="/blog/sales-data-analysis/Aspose.Words.3f482350-c406-414e-b02b-91d71b09cc1b.010.webp"
      alt="A picture containing text, screenshot, diagram, plot Description automatically generated"
      loading="lazy"
      >
  
  
</figure></p>
<p>Again, this graph makes sense since most of the audience is represented by the centre curve, which gives the appearance of a normal curve. Most of the audience is between the ages of 26 and 35, while young people and the elderly have the lowest levels of purchasing power.</p>
<p><strong>CODE</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># Check the purchase distribution with respect to City_Category</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>purchase_city <span style="color:#f92672">=</span> sales_df<span style="color:#f92672">.</span>groupby[<span style="color:#e6db74">&#39;City_Category&#39;</span>](<span style="color:#e6db74">&#39;Purchase&#39;</span>)<span style="color:#f92672">.</span>sum() <span style="color:#f92672">/</span> <span style="color:#ae81ff">1000000</span>
</span></span><span style="display:flex;"><span>purchase_city<span style="color:#f92672">.</span>index <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;A&#39;</span>, <span style="color:#e6db74">&#39;B&#39;</span>, <span style="color:#e6db74">&#39;C&#39;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Plot the purchase distribution by City Category as a pie chart</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>pie(purchase_city, labels<span style="color:#f92672">=</span>purchase_city<span style="color:#f92672">.</span>index, autopct<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;</span><span style="color:#e6db74">%1.1f%%</span><span style="color:#e6db74">&#39;</span>, startangle<span style="color:#f92672">=</span><span style="color:#ae81ff">90</span>, colors<span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;#4C72B0&#39;</span>, <span style="color:#e6db74">&#39;#55A868&#39;</span>, <span style="color:#e6db74">&#39;#C44E52&#39;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Add a title</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Purchase Distribution by City Category&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Add a circle at the center of the pie to make it donut-like</span>
</span></span><span style="display:flex;"><span>circle <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>Circle((<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>), <span style="color:#ae81ff">0.10</span>, fc<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;white&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>gca()<span style="color:#f92672">.</span>add_artist(circle)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>axis(<span style="color:#e6db74">&#39;equal&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Display the plot</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><strong>OUTPUT</strong></p>
<p>






<figure>
  
    <img
      sizes="100vw"
      src="/blog/sales-data-analysis/Aspose.Words.3f482350-c406-414e-b02b-91d71b09cc1b.011.webp"
      alt="output-ss"
      loading="lazy"
      >
  
  
</figure></p>
<p>We can determine the percentage distribution of the city category by plotting a pie chart, and this helps us conclude that people from category B cities have the greatest purchasing power and willingness to spend, while category A cities have the fewest purchases made‚Äînearly 1.8 times fewer than city type B.</p>
<p><strong>CODE</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># Check the purchase distribution with respect to Occupation</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>purchase_occupation <span style="color:#f92672">=</span> sales_df<span style="color:#f92672">.</span>groupby[<span style="color:#e6db74">&#39;Occupation&#39;</span>](<span style="color:#e6db74">&#39;Purchase&#39;</span>)<span style="color:#f92672">.</span>sum() <span style="color:#f92672">/</span> <span style="color:#ae81ff">1000000</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Bar plot</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>bar(purchase_occupation<span style="color:#f92672">.</span>index, purchase_occupation<span style="color:#f92672">.</span>values, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;#4C72B0&#39;</span>, edgecolor<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;black&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Set labels and title</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Purchase Distribution by Occupation&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Occupation Category&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;Purchase (in millions)&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Add data labels on top of the bars</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i, value <span style="color:#f92672">**</span><span style="color:#f92672">in</span><span style="color:#f92672">**</span> enumerate(purchase_occupation<span style="color:#f92672">.</span>values):
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>text(i, value <span style="color:#f92672">+</span> <span style="color:#ae81ff">0.2</span>, <span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;</span><span style="color:#e6db74">{</span>value<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>, ha<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;center&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Add a background color to the plot</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>gca()<span style="color:#f92672">.</span>set_facecolor(<span style="color:#e6db74">&#39;#F5F5F5&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Adjust spacing</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>tight_layout()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Display the plot</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><strong>OUTPUT</strong></p>
<p>






<figure>
  
    <img
      sizes="100vw"
      src="/blog/sales-data-analysis/Aspose.Words.3f482350-c406-414e-b02b-91d71b09cc1b.012.webp"
      alt="A picture containing text, screenshot, line, diagram Description automatically generated"
      loading="lazy"
      >
  
  
</figure></p>
<p>As various employment categories pay varying incomes, it stands to reason that category 0, 4, and 7 occupations may potentially have the greatest earnings, which would therefore likely result in more expenditure on goods and services.</p>
<p>But the lowest purchase amounts are made by jobs in categories 8, 9, and 18.</p>
<p>This may be because individuals in these jobs don&rsquo;t require the items made by business ABC, or it may be because of the extremely low earnings in the industrial sector.</p>
<p><strong>CODE</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># Check the purchase distribution with respect to Stay_In_Current_City_Years</span>
</span></span><span style="display:flex;"><span>purchase_stay <span style="color:#f92672">=</span> sales_df<span style="color:#f92672">.</span>groupby[<span style="color:#e6db74">&#39;Stay_In_Current_City_Years&#39;</span>](<span style="color:#e6db74">&#39;Purchase&#39;</span>)<span style="color:#f92672">.</span>sum() <span style="color:#f92672">/</span> <span style="color:#ae81ff">1000000</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Bar plot</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>bar(purchase_stay<span style="color:#f92672">.</span>index, purchase_stay<span style="color:#f92672">.</span>values, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;#8B008B&#39;</span>, edgecolor<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;black&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Set labels and title</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Purchase Distribution by Stay in Current City Years&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Years stayed in Current City&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;Purchase (in millions)&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Add data labels on top of the bars</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">**</span><span style="color:#66d9ef">for</span><span style="color:#f92672">**</span> i, value <span style="color:#f92672">**</span><span style="color:#f92672">in</span><span style="color:#f92672">**</span> enumerate(purchase_stay<span style="color:#f92672">.</span>values):
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">``</span>plt<span style="color:#f92672">.</span>text(i, value <span style="color:#f92672">+</span> <span style="color:#ae81ff">0.2</span>, <span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;</span><span style="color:#e6db74">{</span>value<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>, ha<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;center&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>gca()<span style="color:#f92672">.</span>set_facecolor(<span style="color:#e6db74">&#39;#F5F5F5&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>tight_layout()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Display the plot</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><strong>OUTPUT</strong></p>
<p>






<figure>
  
    <img
      sizes="100vw"
      src="/blog/sales-data-analysis/Aspose.Words.3f482350-c406-414e-b02b-91d71b09cc1b.013.webp"
      alt="A picture containing text, screenshot, diagram, rectangle Description automatically generated"
      loading="lazy"
      >
  
  
</figure></p>
<p>This bar graph reflects the needs of newcomers to the city who require products and services to establish themselves. As a result, those who have been in the city for a year have made the most purchases, which quickly decline as they continue to live there.</p>
<p><strong>CODE</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># check the purchase distribution with respect to Marital_Status</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>purchase_marital <span style="color:#f92672">=</span> sales_df<span style="color:#f92672">.</span>groupby[<span style="color:#e6db74">&#39;Marital_Status&#39;</span>](<span style="color:#e6db74">&#39;Purchase&#39;</span>)<span style="color:#f92672">.</span>sum() <span style="color:#f92672">/</span> <span style="color:#ae81ff">1000000</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Map values of Marital_Status to corresponding labels</span>
</span></span><span style="display:flex;"><span>purchase_marital<span style="color:#f92672">.</span>index <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;Unmarried&#39;</span>, <span style="color:#e6db74">&#39;Married&#39;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Bar plot</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>bar(purchase_marital<span style="color:#f92672">.</span>index, purchase_marital<span style="color:#f92672">.</span>values, color<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;#6495ED&#39;</span>, <span style="color:#e6db74">&#39;#FFA07A&#39;</span>] , edgecolor<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;black&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Set labels and title</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Purchase Distribution by Marital Status&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Marital Status&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;Purchase (in millions)&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Add data labels on top of the bars</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i, value <span style="color:#f92672">**</span><span style="color:#f92672">in</span><span style="color:#f92672">**</span> enumerate(purchase_marital<span style="color:#f92672">.</span>values):
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">``</span>plt<span style="color:#f92672">.</span>text(i, value <span style="color:#f92672">+</span> <span style="color:#ae81ff">0.2</span>, <span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;</span><span style="color:#e6db74">{</span>value<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>, ha<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;center&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Add a background color to the plot</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>gca()<span style="color:#f92672">.</span>set_facecolor(<span style="color:#e6db74">&#39;#F5F5F5&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Adjust spacing</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>tight_layout()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Display the plot</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><strong>OUTPUT</strong></p>
<p>






<figure>
  
    <img
      sizes="100vw"
      src="/blog/sales-data-analysis/Aspose.Words.3f482350-c406-414e-b02b-91d71b09cc1b.014.webp"
      alt="A picture containing text, screenshot, diagram, plot Description automatically generated"
      loading="lazy"
      >
  
  
</figure></p>
<p>When contrasted to married people, the bias obviously favours single people, probably because of the costs associated with education, travel, and other products and services. Nonetheless, because of financial considerations like family planning and long-term savings, married people may tend to spend less.</p>
<p>A <strong>scatter plot</strong> is a type of data visualization that represents the relationship between two variables. It displays individual data points as dots on a two-dimensional graph, with one variable plotted along the x-axis and the other variable plotted along the y-axis. Scatter plots are useful for visualizing the correlation or relationship between two continuous variables.</p>
<p>To see the relationship between product category 1 (categorical) and purchase using a scatter plot, we need to convert the categorical variable into a numerical representation. One way to achieve this is by assigning a numeric code to each category. For example, we can map &lsquo;Product_Category_1&rsquo; values like &lsquo;Category A&rsquo; to 1, &lsquo;Category B&rsquo; to 2, and so on.</p>
<p><strong>CODE</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># Scatter plot</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>scatter(sales_df[<span style="color:#e6db74">&#39;Product_Category_1&#39;</span>], sales_df[<span style="color:#e6db74">&#39;Purchase&#39;</span>], color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;#8A2BE2&#39;</span>, edgecolors<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;black&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Set labels and title</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Product Category 1&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;Purchase&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Relationship between Purchase and Product Category 1&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Add gridlines</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>grid(<span style="color:#66d9ef">True</span>, linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;--&#39;</span>, linewidth<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.7</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Add transparency to the scatter points</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>scatter(sales_df[<span style="color:#e6db74">&#39;Product_Category_1&#39;</span>], sales_df[<span style="color:#e6db74">&#39;Purchase&#39;</span>], color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;#8A2BE2&#39;</span>,<span style="color:#f92672">**</span> edgecolors<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;black&#39;</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>legend([<span style="color:#e6db74">&#39;Data Points&#39;</span>])
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>gca()<span style="color:#f92672">.</span>set_facecolor(<span style="color:#e6db74">&#39;#F5F5F5&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>tight_layout()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Display the plot</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><strong>OUTPUT</strong></p>
<p>






<figure>
  
    <img
      sizes="100vw"
      src="/blog/sales-data-analysis/Aspose.Words.3f482350-c406-414e-b02b-91d71b09cc1b.015.webp"
      alt="A picture containing text, screenshot, diagram Description automatically generated"
      loading="lazy"
      >
  
  
</figure></p>
<p>As seen above, product categories 4, 13, 19 and 20 have products which are not priced more than 5000 hence these product categories, although numerous will generate the least revenue. On the other hand, product categories 9, 10, 6, and 7 have items with prices that typically start at or exceed 5,000 and can even reach 20,000 or 25,000, generating more revenue for the business. You can also probabilisticlaly determine the name of these categories from the price ranges. Most other categories are somewhere in between.</p>
<p><strong>CODE</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># Scatter plot</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>scatter(sales_df[<span style="color:#e6db74">&#39;Product_Category_2&#39;</span>], sales_df[<span style="color:#e6db74">&#39;Purchase&#39;</span>], color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;#32CD32&#39;</span> , edgecolors<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;black&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Set labels and title</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Product Category 2&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;Purchase&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Relationship between Purchase and Product Category 2&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Add gridlines</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>grid(<span style="color:#66d9ef">True</span>, linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;--&#39;</span>, linewidth<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.7</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Add transparency to the scatter points</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>scatter(sales_df[<span style="color:#e6db74">&#39;Product_Category_2&#39;</span>], sales_df[<span style="color:#e6db74">&#39;Purchase&#39;</span>], color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;#32CD32&#39;</span> , edgecolors<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;black&#39;</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Add a legend</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>legend([<span style="color:#e6db74">&#39;Data Points&#39;</span>])
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>gca()<span style="color:#f92672">.</span>set_facecolor(<span style="color:#e6db74">&#39;#F5F5F5&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>tight_layout()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Display the plot</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><strong>OUTPUT</strong></p>
<p>






<figure>
  
    <img
      sizes="100vw"
      src="/blog/sales-data-analysis/Aspose.Words.3f482350-c406-414e-b02b-91d71b09cc1b.016.webp"
      alt="output-ss"
      loading="lazy"
      >
  
  
</figure></p>
<p><strong>CODE</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># Plot the most frequently bought products</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">5</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>bar(top_frequently_bought<span style="color:#f92672">.</span>index, top_frequently_bought<span style="color:#f92672">.</span>values, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;#4287f5&#39;</span>, edgecolor<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;black&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Most Frequently Bought Products&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Product ID&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;Purchase Count&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xticks(rotation<span style="color:#f92672">=</span><span style="color:#ae81ff">90</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>tight_layout()  <span style="color:#75715e"># Adjust spacing</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Plot the products that generated the most revenue</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">5</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>bar(top_revenue_products<span style="color:#f92672">.</span>index, top_revenue_products<span style="color:#f92672">.</span>values<span style="color:#f92672">/</span><span style="color:#ae81ff">1000000</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;#4287f5&#39;</span>, edgecolor<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;black&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Products with the Most Revenue&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Product ID&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;Revenue (in millions)&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xticks(rotation<span style="color:#f92672">=</span><span style="color:#ae81ff">90</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>tight_layout()  <span style="color:#75715e"># Adjust spacing</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><strong>OUTPUT</strong></p>
<p>






<figure>
  
    <img
      sizes="100vw"
      src="/blog/sales-data-analysis/Aspose.Words.3f482350-c406-414e-b02b-91d71b09cc1b.017.webp"
      alt="A picture containing text, screenshot, line, rectangle Description automatically generated"
      loading="lazy"
      >
  
  
</figure></p>
<p>






<figure>
  
    <img
      sizes="100vw"
      src="/blog/sales-data-analysis/Aspose.Words.3f482350-c406-414e-b02b-91d71b09cc1b.018.webp"
      alt="A picture containing text, screenshot, rectangle, line Description automatically generated"
      loading="lazy"
      >
  
  
</figure></p>
<p>This code is plotting two bar charts. The first chart shows the most frequently bought products, displaying the purchase count for each product. The second chart displays the products that generated the most revenue, showing the revenue (in millions) for each product.</p>
<p>These visualizations provide valuable insights into the popular products and their revenue contribution, which can help identify best-selling items and inform business decisions related to inventory management, marketing strategies, and product promotions. This will prove beneficial for the company to analyze their sales data and identify which products are most popular among their customers.</p>
<h2 id="label-encoding-and-feature-selection">Label Encoding and Feature Selection<a hidden class="anchor" aria-hidden="true" href="#label-encoding-and-feature-selection">#</a></h2>
<p>From this section we will be cleaning up the dataset and picking the features and the target for our Machine Learning model. But before moving on, let us describe a few terms.</p>
<p><strong>Label encoding</strong> is a process of converting categorical variables into numerical format. It assigns a unique numeric label to each category within a variable. This encoding is useful when working with machine learning algorithms that require numerical inputs.</p>
<p><strong>Feature selection</strong> is the process of selecting a subset of relevant features (variables) from the available dataset that are most predictive or informative for the target variable. It helps to improve model performance, reduce overfitting, and enhance interpretability.</p>
<p>In machine learning models, the &ldquo;<strong>target</strong>&rdquo; refers to the variable that the model aims to predict or estimate. It is also known as the dependent variable or the output variable. The target variable represents the outcome or the value we want to predict based on the input features (independent variables) in the model. The model learns patterns and relationships in the input features to make predictions or classifications for the target variable.</p>
<p><strong>The steps involved in making a robust ML model are:</strong></p>
<p>To set up your machine learning algorithm for predicting the values of the &ldquo;purchase&rdquo; column based on the given train.csv and test.csv datasets, you can follow these steps:</p>
<p><strong>Data Preprocessing</strong></p>
<ul>
<li>Load the train.csv dataset and perform necessary data cleaning and preprocessing steps such as handling missing values, encoding categorical variables, and splitting the data into features (X_train) and target (y_train).</li>
<li>Similarly, preprocess the test.csv dataset, ensuring that it undergoes the same preprocessing steps as the training data. However, since the &ldquo;purchase&rdquo; column is missing in the test dataset, you can exclude it from the features (X_test) and treat it as the target variable that you want to predict.</li>
</ul>
<p><strong>Feature Selection and Engineering:</strong></p>
<ul>
<li>Based on the analysis of the data and any available correlation insights, select the relevant features that have a strong impact on the target variable.</li>
<li>Perform any feature engineering techniques such as creating new features, scaling/normalizing the data, or transforming variables if necessary. Ensure that these steps are consistently applied to both the training and test datasets.</li>
</ul>
<p><strong>Model Selection and Training:</strong></p>
<ul>
<li>Choose an appropriate machine learning algorithm for regression, such as Linear Regression, Random Forest Regression, or Gradient Boosting Regression.</li>
<li>Split the preprocessed training data (X_train and y_train) into training and validation sets.</li>
<li>Train your chosen model on the training data and tune hyperparameters if necessary, using techniques like cross-validation and grid search.</li>
</ul>
<p><strong>Model Evaluation:</strong></p>
<ul>
<li>Evaluate the performance of your trained model on the validation set using appropriate evaluation metrics such as mean squared error (MSE), root mean squared error (RMSE), or R-squared. This will give you an idea of how well your model is performing.</li>
</ul>
<p><strong>Model Prediction:</strong></p>
<ul>
<li>Once you are satisfied with the model&rsquo;s performance, use it to predict the &ldquo;purchase&rdquo; values for the preprocessed test dataset (X_test) that does not have the &ldquo;purchase&rdquo; column.</li>
<li>The predicted values will serve as the predicted purchase amounts for each customer and product combination in the test dataset.</li>
</ul>
<p><strong>CODE</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#39;test.csv&#39;</span> , <span style="color:#e6db74">&#39;r&#39;</span>) <span style="color:#66d9ef">as</span> f:
</span></span><span style="display:flex;"><span>    test <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_csv(f)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Instantiate the LabelEncoder</span>
</span></span><span style="display:flex;"><span>le <span style="color:#f92672">=</span> LabelEncoder()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_ml <span style="color:#f92672">=</span> sales_df<span style="color:#f92672">.</span>copy()
</span></span><span style="display:flex;"><span>test_ml <span style="color:#f92672">=</span> test<span style="color:#f92672">.</span>copy()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_ml[<span style="color:#e6db74">&#39;User_ID&#39;</span>]<span style="color:#f92672">=</span>le<span style="color:#f92672">.</span>fit_transform(train_ml[<span style="color:#e6db74">&#39;User_ID&#39;</span>])
</span></span><span style="display:flex;"><span>test_ml[<span style="color:#e6db74">&#39;User_ID&#39;</span>]<span style="color:#f92672">=</span>le<span style="color:#f92672">.</span>fit_transform(test_ml[<span style="color:#e6db74">&#39;User_ID&#39;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_ml[<span style="color:#e6db74">&#39;Product_ID&#39;</span>]<span style="color:#f92672">=</span>le<span style="color:#f92672">.</span>fit_transform(train_ml[<span style="color:#e6db74">&#39;Product_ID&#39;</span>])
</span></span><span style="display:flex;"><span>test_ml[<span style="color:#e6db74">&#39;Product_ID&#39;</span>]<span style="color:#f92672">=</span>le<span style="color:#f92672">.</span>fit_transform(test_ml[<span style="color:#e6db74">&#39;Product_ID&#39;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_ml[<span style="color:#e6db74">&#39;Age&#39;</span>]<span style="color:#f92672">=</span>train_ml[<span style="color:#e6db74">&#39;Age&#39;</span>]<span style="color:#f92672">.</span>map({<span style="color:#e6db74">&#39;0-17&#39;</span>:<span style="color:#ae81ff">17</span>,<span style="color:#e6db74">&#39;55+&#39;</span>:<span style="color:#ae81ff">60</span>,<span style="color:#e6db74">&#39;26-35&#39;</span>:<span style="color:#ae81ff">35</span>, <span style="color:#e6db74">&#39;46-50&#39;</span>:<span style="color:#ae81ff">50</span>,<span style="color:#e6db74">&#39;51-55&#39;</span>:<span style="color:#ae81ff">55</span>,<span style="color:#e6db74">&#39;36-45&#39;</span>:<span style="color:#ae81ff">45</span>,<span style="color:#e6db74">&#39;18-25&#39;</span>:<span style="color:#ae81ff">25</span>})
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>test_ml[<span style="color:#e6db74">&#39;Age&#39;</span>]<span style="color:#f92672">=</span>test_ml[<span style="color:#e6db74">&#39;Age&#39;</span>]<span style="color:#f92672">.</span>map({<span style="color:#e6db74">&#39;0-17&#39;</span>:<span style="color:#ae81ff">17</span>,<span style="color:#e6db74">&#39;55+&#39;</span>:<span style="color:#ae81ff">60</span>,<span style="color:#e6db74">&#39;26-35&#39;</span>:<span style="color:#ae81ff">35</span>,<span style="color:#e6db74">&#39;46-50&#39;</span>:<span style="color:#ae81ff">50</span>,<span style="color:#e6db74">&#39;51-55&#39;</span>:<span style="color:#ae81ff">55</span>,<span style="color:#e6db74">&#39;36-45&#39;</span>:<span style="color:#ae81ff">45</span>,<span style="color:#e6db74">&#39;18-25&#39;</span>:<span style="color:#ae81ff">25</span>})
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_ml[<span style="color:#e6db74">&#39;Stay_In_Current_City_Years&#39;</span>]<span style="color:#f92672">=</span>train_ml[<span style="color:#e6db74">&#39;Stay_In_Current_City_Years&#39;</span>]<span style="color:#f92672">.</span>map({<span style="color:#e6db74">&#39;2&#39;</span>:<span style="color:#ae81ff">2</span>,<span style="color:#e6db74">&#39;4+&#39;</span>:<span style="color:#ae81ff">4</span>,
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;3&#39;</span>:<span style="color:#ae81ff">3</span>,<span style="color:#e6db74">&#39;1&#39;</span>:<span style="color:#ae81ff">1</span>,<span style="color:#e6db74">&#39;0&#39;</span>:<span style="color:#ae81ff">0</span>})
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>test_ml[<span style="color:#e6db74">&#39;Stay_In_Current_City_Years&#39;</span>]<span style="color:#f92672">=</span>test_ml[<span style="color:#e6db74">&#39;Stay_In_Current_City_Years&#39;</span>]<span style="color:#f92672">.</span>map({<span style="color:#e6db74">&#39;2&#39;</span>:<span style="color:#ae81ff">2</span>,<span style="color:#e6db74">&#39;4+&#39;</span>:<span style="color:#ae81ff">4</span>,
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;3&#39;</span>:<span style="color:#ae81ff">3</span>,<span style="color:#e6db74">&#39;1&#39;</span>:<span style="color:#ae81ff">1</span>,<span style="color:#e6db74">&#39;0&#39;</span>:<span style="color:#ae81ff">0</span>})
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>category_train_ml<span style="color:#f92672">=</span>train_ml<span style="color:#f92672">.</span>select_dtypes(include<span style="color:#f92672">=</span>[object])<span style="color:#f92672">.</span>columns
</span></span><span style="display:flex;"><span>le<span style="color:#f92672">=</span>LabelEncoder()
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> col <span style="color:#f92672">in</span> category_train_ml:
</span></span><span style="display:flex;"><span>    train_ml[col]<span style="color:#f92672">=</span>le<span style="color:#f92672">.</span>fit_transform(train_ml[col])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>categorical_test_ml<span style="color:#f92672">=</span>test_ml<span style="color:#f92672">.</span>select_dtypes(include<span style="color:#f92672">=</span>[object])<span style="color:#f92672">.</span>columns
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> cols <span style="color:#f92672">in</span> categorical_test_ml:
</span></span><span style="display:flex;"><span>    test_ml[cols]<span style="color:#f92672">=</span>le<span style="color:#f92672">.</span>fit_transform(test_ml[cols])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_ml<span style="color:#f92672">.</span>tail()
</span></span></code></pre></div><p><strong>OUTPUT</strong></p>
<pre tabindex="0"><code>User_ID  Product_ID  Gender  Age  Occupation  City_Category  
550063     5883        3567       1   55          13              1
550064     5885        3568       0   35           1              2
550065     5886        3568       0   35          15              1
550066     5888        3568       0   60           1              2
550067     5889        3566       0   50           0              1

Stay_In_Current_City_Years  Marital_Status  Product_Category_1  
550063                           1               1                  20
550064                           3               0                  20
550065                           4               1                  20
550066                           2               0                  20
550067                           4               1                  20

Product_Category_2  Product_Category_3  Purchase  
550063            9.842329           12.668243       368  
550064            9.842329           12.668243       371  
550065            9.842329           12.668243       137  
550066            9.842329           12.668243       365  
550067            9.842329           12.668243       490  
</code></pre><p>Here‚Äôs what we did above, as you can guess from the output.</p>
<ol>
<li>The test.csv file is being read using pandas‚Äô read_csv function and stored in a variable called test. Previously we worked only with train.csv but since we are now building the actual ML model, we will need the test data set as well.</li>
<li>An instance of the LabelEncoder class is being created and stored in a variable called le.</li>
<li>The sales_df dataframe is being copied into two new dataframes called train_ml and test_ml.</li>
<li>The User_ID and Product_ID columns of both train_ml and test_ml is being encoded using the fit_transform method of le.</li>
<li>The Age column of both train_ml and test_ml is being mapped to new values using a dictionary.</li>
<li>The Stay_In_Current_City_Years column of both train_ml and test_ml is being mapped to new values using a dictionary.</li>
<li>All categorical columns of train_ml is being encoded using the fit_transform method of le.</li>
<li>All categorical columns of test_ml is being encoded using the fit_transform method of le.</li>
</ol>
<p><strong>To improve Feature Selection, the following steps will prove beneficial.</strong></p>
<ol>
<li>Load and preprocess your dataset.</li>
<li>Split the dataset into input features (X) and the target variable (y).</li>
<li>Import the necessary libraries for feature selection.</li>
<li>Apply one or more feature selection techniques to evaluate the importance of each feature.</li>
<li>Select the top k features based on their importance scores or other criteria.</li>
<li>Subset your dataset to include only the selected features.</li>
<li>Train your model using the subset of selected features.</li>
<li>Evaluate the performance of your model using appropriate metrics.</li>
</ol>
<p>Improving feature selection involves identifying the most relevant and informative features for your prediction task. Here are some approaches to improve feature selection:</p>
<ol>
<li>
<p><strong>Univariate Feature Selection</strong>: Use statistical tests or metrics to evaluate the relationship between each feature and the target variable independently. Select the features with the highest scores or p-values as the most relevant.</p>
</li>
<li>
<p><strong>Recursive Feature Elimination</strong>: Train a model using all features and recursively eliminate the least important features based on their coefficients or feature importances. This iterative process helps identify the subset of features that contribute the most to the model&rsquo;s performance.</p>
</li>
<li>
<p><strong>Feature Importance from Tree-based Models</strong>: Train tree-based models such as Random Forest or XGBoost and extract the feature importances. Select the features with the highest importances as they have a greater impact on the model&rsquo;s predictions.</p>
</li>
<li>
<p><strong>Regularization Techniques</strong>: Use regularization techniques like L1 (Lasso) or L2 (Ridge) regularization to penalize less important features and encourage sparsity. These techniques can help automatically select the most informative features.</p>
</li>
<li>
<p><strong>Domain Knowledge and Feature Engineering</strong>: Leverage your domain knowledge to engineer new features or transform existing ones that may provide more relevant information for the prediction task. Feature engineering can significantly improve the performance of your model.</p>
</li>
<li>
<p><strong>Dimensionality Reduction Techniques</strong>: Apply dimensionality reduction techniques like Principal Component Analysis (PCA) or Singular Value Decomposition (SVD) to reduce the dimensionality of the feature space while retaining most of the important information. This can help eliminate redundant or less informative features.</p>
</li>
<li>
<p><strong>Regular Monitoring and Iterative Improvement</strong>: Continuously monitor the performance of your model and iterate on feature selection. Experiment with different combinations of features, feature transformations, and feature engineering techniques to find the most effective set of features.</p>
</li>
</ol>
<h2 id="building-the-ml-model">Building the ML Model<a hidden class="anchor" aria-hidden="true" href="#building-the-ml-model">#</a></h2>
<p>After completing the data preprocessing, analysis, visualization, and label encoding, we are now ready to build a machine learning model to predict the &ldquo;Purchase&rdquo; value for company ABC. In this case, we are dealing with a regression problem since we want to estimate a continuous numerical value.</p>
<p>Among the various regression models available, one of the top models provided by the scikit-learn library is the Linear Regression model. Linear Regression is a popular and widely used regression technique that assumes a linear relationship between the input features and the target variable. It aims to find the best-fit line that minimizes the difference between the actual and predicted values.</p>
<p>The Linear Regression model in scikit-learn provides various functionalities, including:</p>
<ol>
<li>Handling multiple input features and calculating their coefficients.</li>
<li>Performing feature scaling to standardize the input features.</li>
<li>Handling categorical variables using techniques like one-hot encoding or label encoding.</li>
<li>Evaluating the model&rsquo;s performance using various metrics such as mean squared error (MSE), root mean squared error (RMSE), mean absolute error (MAE), and coefficient of determination (R-squared). These metrics help us assess how well the model fits the data and how accurate its predictions are.</li>
</ol>
<p>To select the best regression model for our task, we will train and evaluate multiple regression models, such as Linear Regression, Decision Tree Regression, Random Forest Regression, and XGBoost Regression and evaluate their performance to see which yields the highest accuracy.</p>
<p>Before moving on to the coding phase, let us first identify the metrics to identify the best performing model.</p>
<ol>
<li><strong>R2 score</strong>, also known as the coefficient of determination, measures the proportion of the variance in the target variable that can be explained by the model. It indicates how well the model fits the data, with a <strong>higher value indicating a better fit.</strong></li>
<li><strong>MSE</strong> calculates the average squared difference between the actual and predicted values, while <strong>RMSE</strong> is the square root of MSE, providing a more interpretable metric in the original unit of the target variable.</li>
<li><strong>Lower MSE and RMSE values indicate better accuracy and less error in the predictions.</strong></li>
</ol>
<p><strong>LINEAR REGRESSION</strong></p>
<p>It is a simple and widely used regression algorithm that assumes a linear relationship between the input features and the target variable. It calculates the coefficients for each feature to fit a best-fit line to the data. It is easy to interpret and provides insights into the impact of each feature on the target variable.</p>
<p><strong>CODE</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>x<span style="color:#f92672">=</span> train_ml<span style="color:#f92672">.</span>drop([<span style="color:#e6db74">&#39;Purchase&#39;</span>],axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>y<span style="color:#f92672">=</span> train_ml[<span style="color:#e6db74">&#39;Purchase&#39;</span>]
</span></span><span style="display:flex;"><span>test_x<span style="color:#f92672">=</span>test_ml
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_x,val_x,train_y,val_y<span style="color:#f92672">=</span>train_test_split(x,y,test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>,random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>,shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># LINEAR REGRESSION</span>
</span></span><span style="display:flex;"><span>lr<span style="color:#f92672">=</span>LinearRegression()
</span></span><span style="display:flex;"><span>lr_model<span style="color:#f92672">=</span>lr<span style="color:#f92672">.</span>fit(train_x,train_y)
</span></span><span style="display:flex;"><span>pred_lr<span style="color:#f92672">=</span>lr_model<span style="color:#f92672">.</span>predict(val_x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>mse <span style="color:#f92672">=</span> mean_squared_error(pred_lr, val_y)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Linear REG Mean Square Error: &#34;</span>, mse)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>rmse_lr <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sqrt(mean_squared_error(val_y, pred_lr))
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Linear REG Root Mean Square Error: &#34;</span>, rmse_lr)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>features_lr <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>columns
</span></span><span style="display:flex;"><span>coeff_lr <span style="color:#f92672">=</span> lr_model<span style="color:#f92672">.</span>coef_
</span></span><span style="display:flex;"><span>coefficients_lr <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>Series(lr_model<span style="color:#f92672">.</span>coef_, features_lr)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">6</span>))
</span></span><span style="display:flex;"><span>sns<span style="color:#f92672">.</span>barplot(x<span style="color:#f92672">=</span>coeff_lr, y<span style="color:#f92672">=</span>features_lr, palette<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Blues_r&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#34;Linear Regression Coefficients&#34;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;Coefficient&#34;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">12</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;Feature&#34;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">12</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xticks(fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>yticks(fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>tight_layout()
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><strong>OUTPUT</strong></p>
<pre tabindex="0"><code>Linear REG Mean Square Error:  21708175.443769183
Linear REG Root Mean Square Error:  4659.203305691777
</code></pre><p>






<figure>
  
    <img
      sizes="100vw"
      src="/blog/sales-data-analysis/Aspose.Words.3f482350-c406-414e-b02b-91d71b09cc1b.019.webp"
      alt="output-ss"
      loading="lazy"
      >
  
  
</figure></p>
<p><strong>XGBOOST REGRESSION</strong></p>
<p>It is an optimized gradient boosting framework that excels in handling structured data. It is an ensemble model that combines multiple weak learners (decision trees) to make accurate predictions. XGBoost Regression is specifically designed for regression tasks and provides excellent performance and flexibility. It handles missing values, supports regularization techniques, and offers advanced features like early stopping to prevent overfitting.</p>
<p><strong>CODE</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># XGBOOST REGRESSOR</span>
</span></span><span style="display:flex;"><span>XGBoost_Regression <span style="color:#f92672">=</span> XGBRegressor(learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>, max_depth<span style="color:#f92672">=</span><span style="color:#ae81ff">6</span>, min_child_weight<span style="color:#f92672">=</span><span style="color:#ae81ff">40</span>, seed<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>XGBoost_Regression<span style="color:#f92672">.</span>fit(train_x, train_y)
</span></span><span style="display:flex;"><span>pred_xgb <span style="color:#f92672">=</span> XGBoost_Regression<span style="color:#f92672">.</span>predict(val_x)
</span></span><span style="display:flex;"><span>rmse_xgb <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sqrt(mean_squared_error(pred_xgb, val_y))
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;RMSE for XGBoost Regressor:&#34;</span>, rmse_xgb)
</span></span></code></pre></div><p><strong>OUTPUT</strong></p>
<pre tabindex="0"><code>RMSE for XGBoost Regressor: 2591.1169777068635
</code></pre><p>In the context of XGBoost, <strong>n_estimators</strong> is a hyperparameter that represents the number of decision trees to be built in the XGBoost ensemble. Each decision tree is trained sequentially, and the final prediction is obtained by aggregating the predictions of all the trees.</p>
<p>Increasing the value of n_estimators can improve the model&rsquo;s performance up to a certain point. More trees allow the model to capture more complex patterns and relationships in the data, potentially leading to better predictive performance. However, using a very large value for n_estimators can also increase the risk of overfitting the training data and may result in longer training times.</p>
<p>It is common to tune the n_estimators hyperparameter during the model selection and evaluation process. This can be done using techniques such as cross-validation or grid search, where different values of n_estimators are tested to find the optimal value that balances model performance and computational efficiency. We are setting <strong>n_estimators=100</strong> means that the XGBoost model will be trained using 100 decision trees in the ensemble.</p>
<p><strong>RANDOM FOREST REGRESSION</strong></p>
<p>Random Forest Regression is an ensemble model that builds a multitude of decision trees and combines their predictions to obtain a more accurate and robust result. It addresses overfitting and is effective in handling high-dimensional datasets. It provides feature importance scores to identify the most influential features.</p>
<p><strong>CODE</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># RANDOM FOREST REGRESSOR</span>
</span></span><span style="display:flex;"><span>RandomForest_reg<span style="color:#f92672">=</span>RandomForestRegressor(max_depth<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>RandomForest_reg<span style="color:#f92672">.</span>fit(train_x,train_y)
</span></span><span style="display:flex;"><span>RandomForest_reg<span style="color:#f92672">=</span>RandomForest_reg<span style="color:#f92672">.</span>predict(val_x)
</span></span><span style="display:flex;"><span>rmse<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>sqrt(mean_squared_error(RandomForest_reg,val_y))
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;RMSE for Random Forest:&#34;</span>,rmse)
</span></span></code></pre></div><p><strong>OUTPUT</strong></p>
<pre tabindex="0"><code>RMSE for Random Forest: 4163.747031944405
</code></pre><p><strong>ADA BOOST REGRESSION</strong></p>
<p>AdaBoost Regression is an ensemble model that iteratively improves performance by focusing on the previously misclassified instances. It combines weak learners to create a strong learner, making it suitable for regression tasks. It adapts to the data and assigns higher weights to harder-to-predict instances.</p>
<p><strong>CODE</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># ADA BOOST REGRESSOR</span>
</span></span><span style="display:flex;"><span>ADBBoost_Regression<span style="color:#f92672">=</span>AdaBoostRegressor(n_estimators<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>,random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>ADBBoost_Regression<span style="color:#f92672">.</span>fit(train_x,train_y)
</span></span><span style="display:flex;"><span>pred_adb<span style="color:#f92672">=</span>ADBBoost_Regression<span style="color:#f92672">.</span>predict(val_x)
</span></span><span style="display:flex;"><span>rmse<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>sqrt(mean_squared_error(pred_adb,val_y))
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;RMSE for Adaboost Regressor:&#34;</span>,rmse)
</span></span></code></pre></div><p><strong>OUTPUT</strong></p>
<pre tabindex="0"><code>RMSE for Adaboost Regressor: 3595.007906514239
</code></pre><p><strong>GRADIENT BOOST REGRESSION</strong></p>
<p>Gradient Boosting Regression is another ensemble technique that combines weak learners (decision trees) in a sequential manner. It optimizes a loss function by fitting the subsequent models to the residual errors of the previous models. It is a powerful algorithm that achieves high accuracy by minimizing the loss iteratively.</p>
<p><strong>CODE</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># GRADIENT BOOSTING REGRESSOR</span>
</span></span><span style="display:flex;"><span>GradientBoosting_Regression<span style="color:#f92672">=</span>GradientBoostingRegressor(n_estimators<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>GradientBoosting_Regression<span style="color:#f92672">.</span>fit(train_x,train_y)
</span></span><span style="display:flex;"><span>GradientBoostingRegressor(learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>gbr_predicition<span style="color:#f92672">=</span>GradientBoosting_Regression<span style="color:#f92672">.</span>predict(val_x)
</span></span><span style="display:flex;"><span>rmse<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>sqrt(mean_squared_error(gbr_predicition,val_y))
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;RMSE for Gradient Boosting Regressor:&#34;</span>,rmse)
</span></span></code></pre></div><p><strong>OUTPUT</strong></p>
<pre tabindex="0"><code>RMSE for Gradient Boosting Regressor: 2756.5231625627925
</code></pre><p><strong>CODE</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>RMSE <span style="color:#66d9ef">for</span> Gradient Boosting Regressor: <span style="color:#ae81ff">2756.5231625627925</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> r2_score
</span></span><span style="display:flex;"><span>r2 <span style="color:#f92672">=</span> r2_score(val_y, gbr_predicition)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;R2 Score for Gradient Boosting Regressor:&#34;</span>, r2)
</span></span></code></pre></div><p><strong>OUTPUT</strong></p>
<pre tabindex="0"><code>R2 Score for Gradient Boosting Regressor: 0.6975895276400425
</code></pre><p>Feature Importance is a technique used to determine the relevance or contribution of each feature in the prediction task. It helps identify the columns that are most useful in predicting the target variable. Techniques like permutation importance, Gini importance, or feature importance scores provided by ensemble models like Random Forest or XGBoost library.</p>
<p><strong>CODE</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># Plot the feature importance</span>
</span></span><span style="display:flex;"><span>features <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>columns
</span></span><span style="display:flex;"><span>importances <span style="color:#f92672">=</span> GradientBoosting_Regression<span style="color:#f92672">.</span>feature_importances_
</span></span><span style="display:flex;"><span>indices <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argsort(importances)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">6</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Feature Importances&#39;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>barh(range(len(indices)), importances[indices], color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;#4287f5&#39;</span>, edgecolor<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;black&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>yticks(range(len(indices)), [features[i] <span style="color:#f92672">**</span><span style="color:#66d9ef">for</span><span style="color:#f92672">**</span> i <span style="color:#f92672">**</span><span style="color:#f92672">in</span><span style="color:#f92672">**</span> indices], fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Relative Importance&#39;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">12</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><strong>OUTPUT</strong></p>
<p>






<figure>
  
    <img
      sizes="100vw"
      src="/blog/sales-data-analysis/Aspose.Words.3f482350-c406-414e-b02b-91d71b09cc1b.020.webp"
      alt="A picture containing text, screenshot, number, font Description automatically generated"
      loading="lazy"
      >
  
  
</figure></p>
<p>You can explore other popular and well-known regression techniques to boost accuracy even more. Just note that these are CPU-intensive and training time can go from several minutes to several hours depending on the scale of the dataset.</p>
<ol>
<li>
<p><strong>Support Vector Regression (SVR):</strong> SVR is a powerful algorithm for regression tasks that can handle both linear and non-linear relationships. It uses support vectors to capture the important patterns in the data.</p>
</li>
<li>
<p><strong>Neural Networks</strong>: Deep learning models, such as Multilayer Perceptron (MLP) or Recurrent Neural Networks (RNN), can be effective for regression tasks when you have large amounts of data and complex relationships.</p>
</li>
<li>
<p><strong>Ridge Regression</strong>: Ridge regression is a linear regression technique that incorporates regularization to prevent overfitting and handle multicollinearity. It can be useful when you have many features.</p>
</li>
</ol>
<p>To improve the model&rsquo;s performance, you can try the following techniques:</p>
<ol>
<li><strong>Feature Engineering</strong>: Create new features or transform existing features to provide more meaningful information to the model. For example, you could combine related features, create interaction terms, or apply mathematical transformations to certain variables.</li>
<li><strong>Include More Relevant Features</strong>: Explore other features that may have a significant impact on the target variable. Consider adding additional features based on domain knowledge or further analysis of the data.</li>
<li><strong>Remove Irrelevant Features</strong>: Identify and remove features that do not contribute much to the prediction task. These features may have low correlation with the target variable or exhibit multicollinearity with other features.</li>
<li><strong>Polynomial Regression</strong>: Consider using polynomial regression to capture non-linear relationships between the features and the target variable. This can be achieved by creating polynomial features or using polynomial regression algorithms.</li>
<li><strong>Regularization Techniques</strong>: Apply regularization techniques like Ridge Regression or Lasso Regression to prevent overfitting and improve the model&rsquo;s generalization ability. Regularization helps in reducing the impact of irrelevant or noisy features.</li>
<li><strong>Ensemble Methods:</strong> Explore ensemble methods such as Random Forests or Gradient Boosting. These techniques combine multiple models to make more accurate predictions and can handle complex relationships between features and the target variable.</li>
<li><strong>Hyperparameter Tuning</strong>: Optimize the hyperparameters of your chosen algorithm using techniques like grid search or random search. This involves systematically trying different combinations of hyperparameter values to find the best configuration for your model.</li>
<li><strong>Cross-Validation</strong>: Use cross-validation techniques to better estimate the model&rsquo;s performance and reduce overfitting. This helps ensure that the model&rsquo;s performance is not dependent on a specific train-test split.</li>
<li><strong>Collect More Data</strong>: If possible, collect more data to increase the diversity and quantity of samples available for training. More data can often improve the model&rsquo;s accuracy and generalization.</li>
</ol>
<h2 id="making-the-actual-predictions">Making the Actual Predictions<a hidden class="anchor" aria-hidden="true" href="#making-the-actual-predictions">#</a></h2>
<p><strong>CODE</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># save the model to disk</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> pickle
</span></span><span style="display:flex;"><span>filename <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;finalized_model.sav&#39;</span>
</span></span><span style="display:flex;"><span>pickle<span style="color:#f92672">.</span>dump(GradientBoosting_Regression, open(filename, <span style="color:#e6db74">&#39;wb&#39;</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># load the model from disk</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#39;finalized_model.sav&#39;</span>, <span style="color:#e6db74">&#39;rb&#39;</span>) <span style="color:#66d9ef">as</span> file:
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">``</span>ML_MODEL <span style="color:#f92672">=</span> pickle<span style="color:#f92672">.</span>load(file)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(ML_MODEL)
</span></span></code></pre></div><p><strong>OUTPUT</strong></p>
<pre tabindex="0"><code>GradientBoostingRegressor(learning_rate=1.0, random_state=0)
</code></pre><p><strong>CODE</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>XGBoost_Regression<span style="color:#f92672">.</span>fit(x, y)
</span></span><span style="display:flex;"><span>predict_final <span style="color:#f92672">=</span> XGBoost_Regression<span style="color:#f92672">.</span>predict(test_x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># make a predictions dataframe</span>
</span></span><span style="display:flex;"><span>predictions <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>predictions[<span style="color:#e6db74">&#39;Purchase&#39;</span>] <span style="color:#f92672">=</span> predict_final
</span></span><span style="display:flex;"><span>predictions<span style="color:#f92672">.</span>to_csv(<span style="color:#e6db74">&#39;sales_prediction.csv&#39;</span>, index<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span></code></pre></div><p><strong>OUTPUT</strong></p>
<table>
<thead>
<tr>
<th style="text-align:left">Purchase</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">12624.37</td>
</tr>
<tr>
<td style="text-align:left">13786.86</td>
</tr>
<tr>
<td style="text-align:left">3508.918</td>
</tr>
</tbody>
</table>
<p>The sales prediction CSV has the above format. To examine the projected output for each user, copy and paste this column into the original train.csv and test.csv files. The rows are in the same order as they were in the original files.</p>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>This exercise explored the analysis of customer purchasing habits and predicting their spending on products. By understanding customer behavior and preferences, businesses can offer personalized offers and improve their customer targeting strategies.</p>
<p>Let&rsquo;s recap the sales analysis and prediction:</p>
<ul>
<li>
<p>Understanding customer purchasing habits and analyzing customer behavior is essential for businesses to personalize their offerings and enhance customer satisfaction. Through data preprocessing and exploratory data analysis, you gained valuable insights into the dataset, uncovering patterns and relationships.</p>
</li>
<li>
<p>Addressing outliers and visualizing the data through various techniques provided further understanding of the purchasing patterns and correlations between variables. Label encoding and feature selection helped prepare the data for model building by selecting relevant features that contribute to predicting customer spending.</p>
</li>
<li>
<p>Implementing regression models such as Linear Regression, XGBoost Regression, RandomForest Regression, ADA Boost Regression, and Gradient Boost Regression allowed you to leverage customer characteristics and product categories to predict spending accurately.</p>
</li>
<li>
<p>Evaluating the model&rsquo;s performance using metrics like R2 score, mean square error (MSE), and root mean square error (RMSE) revealed the accuracy and effectiveness of the models. The Gradient Boost Regression model achieved an R2 score of approximately 0.70, indicating that around 70% of the variation in customer spending was explained by the model.</p>
</li>
</ul>
<p>In conclusion, by analyzing customer purchasing habits and building predictive models, you gained valuable insights for tailored marketing strategies and business decision-making. Continuous refinement and improvement of the models, along with techniques like feature engineering, hyperparameter tuning, ensemble methods, cross-validation, and handling imbalanced data, can further enhance the accuracy and performance of the models. These insights and models contribute to maximizing customer satisfaction, optimizing business profitability, and driving success in customer purchasing analysis.</p>


  </div>

  <script async data-uid="3bc620cccf" src="https://atharva-shah.ck.page/3bc620cccf/index.js"></script>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://atharvashah.netlify.app/tags/python/">python</a></li>
      <li><a href="https://atharvashah.netlify.app/tags/tutorial/">tutorial</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://atharvashah.netlify.app/posts/personal/movie-review/june-2023/">
    <span class="title">¬´ Prev</span>
    <br>
    <span>June 2023 Movies: Summer Cinema</span>
  </a>
  <a class="next" href="https://atharvashah.netlify.app/posts/personal/music-review/foghorn-in-dawn/">
    <span class="title">Next ¬ª</span>
    <br>
    <span>Album Review: Foghorn in the Dawn by Ïò§ÎØ∏ÏùºÍ≥± [Omilgop]</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Sales Data Analysis with Python on twitter"
        href="https://twitter.com/intent/tweet/?text=Sales%20Data%20Analysis%20with%20Python&amp;url=https%3a%2f%2fatharvashah.netlify.app%2fposts%2ftech%2fsales-data-analysis%2f&amp;hashtags=python%2ctutorial">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Sales Data Analysis with Python on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fatharvashah.netlify.app%2fposts%2ftech%2fsales-data-analysis%2f&amp;title=Sales%20Data%20Analysis%20with%20Python&amp;summary=Sales%20Data%20Analysis%20with%20Python&amp;source=https%3a%2f%2fatharvashah.netlify.app%2fposts%2ftech%2fsales-data-analysis%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Sales Data Analysis with Python on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fatharvashah.netlify.app%2fposts%2ftech%2fsales-data-analysis%2f&title=Sales%20Data%20Analysis%20with%20Python">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Sales Data Analysis with Python on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fatharvashah.netlify.app%2fposts%2ftech%2fsales-data-analysis%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Sales Data Analysis with Python on whatsapp"
        href="https://api.whatsapp.com/send?text=Sales%20Data%20Analysis%20with%20Python%20-%20https%3a%2f%2fatharvashah.netlify.app%2fposts%2ftech%2fsales-data-analysis%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Sales Data Analysis with Python on telegram"
        href="https://telegram.me/share/url?text=Sales%20Data%20Analysis%20with%20Python&amp;url=https%3a%2f%2fatharvashah.netlify.app%2fposts%2ftech%2fsales-data-analysis%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>
  </footer><div id="disqus_thread"></div>
<script type="application/javascript">
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "https-atharvashah-netlify-app" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
<div id="disqus_thread"></div>
<script type="text/javascript">
    (function() {
        
        
        if (window.location.hostname == "localhost")
            return;

        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        var disqus_shortname = 'https-atharvashah-netlify-app';
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
</article>
<script>
  <!-- SCROLLING PROGRESS BAR -->
  const scrollProgress = document.getElementById('scroll-progress');
  const height = document.documentElement.scrollHeight - document.documentElement.clientHeight;

  window.addEventListener('scroll', () => {
    const scrollTop = window.scrollY || document.documentElement.scrollTop;
    const scrollPercentage = (scrollTop / height) * 100;
    scrollProgress.style.width = `${scrollPercentage}%`;
  });
</script>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://atharvashah.netlify.app/">Atharva Shah</a></span> 
    
    |<span><a href="/privacy-policy/" target="_blank">Privacy Policy</a></span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="AtharvaShah" data-description="Support me on Buy me a coffee!" data-message="Every bit of support counts!" data-color="#5F7FFF" data-position="Right" data-x_margin="18" data-y_margin="18"></script>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function() {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function(e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });
</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function() {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })
</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) {};
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script></body>

</html>
